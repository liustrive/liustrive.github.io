<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>古董Leon</title><link>http://www.liustrive.com/</link><description></description><atom:link href="http://www.liustrive.com/feeds/tech.rss.xml" rel="self"></atom:link><lastBuildDate>Sun, 12 May 2013 12:19:00 +0800</lastBuildDate><item><title>Corona 笔记</title><link>http://www.liustrive.com/pages/2013/05/12/corona-bi-ji.html</link><description>&lt;h3&gt;Corona 初探&lt;/h3&gt;
&lt;p&gt;本周关注了一下另一种号称下一代MapReduce的框架：&lt;strong&gt;Corona&lt;/strong&gt;。它是facebook的开源框架，代码在：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona"&gt;https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Cornna的设计需求与Apache的YARN框架十分相似，它的发布文档：
“&lt;a href="http://www.jorditorres.org/wp-content/uploads/2012/11/CC-MEI-Corona-AntonioRodriguez.pdf"&gt;Under the Hood: Scheduling MapReduce jobs more efficiently with Corona&lt;/a&gt;”&lt;/p&gt;
&lt;p&gt;其中提到，当前Map-Reduce所使用的single Job Tracker在Facebook的运行环境下已经遇到瓶颈，在Hadoop Corona中，集群的资源是由central Cluster Manager统一分配调度，每个job拥有各自单独的Corona Job Tracker，这些与YARN框架的设计思路非常相近，解决了Map-Reduce中存在的扩展性、单点故障等不足。&lt;/p&gt;
&lt;p&gt;相比于YARN，Corona似乎更贴近Map-Reduce的原版，其代码在原版Map-Reduce基础上修改而来，似乎更贴近原有Map-Reduce的用户；另一大优势在于Corona所采用的Push-based通信机制，在原版Map-Reduce和YARN中，采用轮询方式使client获取信息，比如心跳机制，当规模过大后，会产生一定延迟，而Corona采用Push-based通信机制，server直接把信息推送至client，为实现这一通信机制，Corona似乎采用了更多的通信协议（待考证）。&lt;/p&gt;
&lt;p&gt;Corona相比与Yarn的缺点也是由于它更贴近原版所产生，比如Corona不能提供Yarn的用户设置所需内存、CPU等特性；任务调度中仍采用slot作为资源分配元，资源利用率不如Yarn；Corona也不支持Yarn那种灵活的多计算框架支持，因为它的TaskTracker还是执行Map和Reduce task。
总的来说，Corona更像是对原版Map-Reduce的升级，从代码量上也可见一斑，Yarn的&lt;a href="http://www.fightrice.com/mirrors/apache//hadoop/core/"&gt;代码&lt;/a&gt; 十五万行代码完全重构，与原版Map-Reduce没有任何复用，而Corona相对原版有大量的重复，更接近与改进了部分框架的升级。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corona说明书上宣称的特点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better scalability and cluster utilization&lt;/li&gt;
&lt;li&gt;Lower latency for small jobs&lt;/li&gt;
&lt;li&gt;Ability to upgrade without disruption&lt;/li&gt;
&lt;li&gt;Scheduling based on actual task resource requirements rather than a count of map and reduce tasks &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Corona的基本组件&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Corona架构&lt;/strong&gt;：
&lt;img alt="Corona" src="http://i.imgur.com/xNagLob.jpg" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Cluster Manager&lt;/strong&gt; 类似于YARN中的Resource Manager，负责资源分配和调度。Cluster Manager掌握着各个节点的资源使用情况，并将资源分配给各个作业（默认调度器为Fair Scheduler）。同YARN中的Resource Manager一样，Resource Manager是一个高度抽象的资源统一分配与调度框架，它不仅可以为MapReduce，也可以为其他计算框架分配资源。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Corona Job Tracker&lt;/strong&gt; 类似于YARN中的Application Master，用于作业的监控和容错，它可以运行在两个模式下：1） 作为JobClient，用于提交作业和方便用户跟踪作业运行状态 2）   作为一个Task运行在某个TaskTracker上。与MRv1中的Job Tracker不同，每个Corona Job Tracker只负责监控一个作业。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Corona Task Tracker&lt;/strong&gt; 类似于YARN中的Node Manager，它的实现重用了MRv1中Task Tracker的很多代码，它通过心跳将节点资源使用情况汇报给Cluster Manager，同时会与Corona Job Tracker通信，以获取新任务和汇报任务运行状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proxy Job Tracker&lt;/strong&gt; 用于离线展示一个作业的历史运行信息，包括Counter、metrics、各个任务运行信息等。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Corona框架的搭建（single node）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona"&gt;https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Building&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start in the hadoop-20 parent path and compile the code with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ant&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Dversion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt; &lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;package&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Starting up the cluster&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, set up the required environment and useful aliases.&lt;/p&gt;
&lt;p&gt;source singleNodeHadoop/singleNodeSwitch.sh corona&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If this is the first time, format the namespace in /tmp with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt; &lt;span class="n"&gt;namenode&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;
&lt;span class="n"&gt;Start&lt;/span&gt; &lt;span class="n"&gt;HDFS&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;
&lt;span class="n"&gt;Start&lt;/span&gt; &lt;span class="n"&gt;Corona&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;corona&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are corresponding commands to stop the clusters
    stop-dfs&lt;/p&gt;
&lt;p&gt;stop-corona&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point you will be able to look at the local Corona Cluster Manager UI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Job&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;export&lt;/span&gt; &lt;span class="n"&gt;HADOOP_CLASSPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;HADOOP_HOME&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;corona&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libthrift&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.7.0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;hadoop&lt;/span&gt; &lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;sleep&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Dmapred&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fairscheduler&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;group_a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pool_sla&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Leon</dc:creator><pubDate>Sun, 12 May 2013 12:19:00 +0800</pubDate><guid>tag:www.liustrive.com,2013-05-12:pages/2013/05/12/corona-bi-ji.html</guid><category>Corona</category><category>cloud</category><category>distribution</category></item><item><title>Network Aware Resource Allocation in Distributed Clouds 笔记</title><link>http://www.liustrive.com/pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html</link><description>&lt;p&gt;INFOCOM 2012 &lt;/p&gt;
&lt;p&gt;Mansoor Alicherry, T.V. Lakshman&lt;/p&gt;
&lt;p&gt;Bell Labs. &lt;/p&gt;
&lt;p&gt;这篇文章主要介绍了通讯负载和延迟最小化的高效的resource allocation算法。&lt;/p&gt;
&lt;p&gt;Key contribution：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Develop an efficient 2-approximation algorithm for the optimal selection of data centers in the distributed cloud.&lt;/li&gt;
&lt;li&gt;Use of an optimal algorithm for rack and server selection.&lt;/li&gt;
&lt;li&gt;develop a heuristic for partitioning the requested resources for the task amongst the chosen data centers and racks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;分布式资源调配系统结构：
&lt;img alt="Network Aware Resource Allocation Architecture" src="http://i.imgur.com/FnsGybs.png" /&gt;&lt;/p&gt;
&lt;p&gt;优化的主要目标是减少datacenter之间的traffic，降低communication cost。
Cloud Automation的四个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Datacenter selection&lt;/li&gt;
&lt;li&gt;Request partitioning across datacenters&lt;/li&gt;
&lt;li&gt;Rack, blade and processor selection&lt;/li&gt;
&lt;li&gt;VM placement&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章主要集中于Datacenter selection，即响应用户请求的第一步：找到合适的若干个datacenter来放置合适数量的VM。&lt;/p&gt;
&lt;p&gt;文章提出了近似算法，其主要目标是通过最小路径找到合适的datacenter，并且通过相VM相互间的通讯延迟，避免存在一些性能会严重拉低整体表现的VM。并且算法提供额外的用户约束，如某datacenter中最大、最小VM数量等。&lt;/p&gt;
&lt;p&gt;文章将datacenter选择问题抽象成了一个完全图G = （V,E,w,e）的子图选择问题，即：从完全图中找出一个最长edge最小的子图的问题，称为MINDIAMETER问题。&lt;/p&gt;
&lt;p&gt;需要注意的是这里提出的抽象是针对另一个文章里的MAXCLIQUE 问题改进简化而来，MAXCLIQUE问题是寻找最大clique问题，文中给出的例子如下：
&lt;img alt="Reduction" src="http://i.imgur.com/UU0YP2G.png" /&gt; &lt;/p&gt;
&lt;p&gt;如图中所示，edge上的数字代表link costs，虚线是补全的edge使图成完全图，加粗的线的子图是MINDIAMETER问题的解，这在原图中也满足最大clique。
随后文章提出了近似算法，算法得到最近似的解，该解得到的子图的diameter最多是最优子图的diameter的二倍。&lt;/p&gt;
&lt;p&gt;文章的基础文：&lt;a href="http://dl.acm.org/citation.cfm?id=574848"&gt;Computers and Intractability; A Guide to the Theory of NP-Completeness&lt;/a&gt;，即MAXCLIQUE问题的来源。&lt;/p&gt;
&lt;p&gt;这篇文章提出了考虑网络状况的资源分配算法，其使用的策略跟之前组会提出的相近，划分出一片相互之间延迟最小的子网，把运行任务的VM放在这个子网里。文章提出的近似算法如果能再进一步优化肯定是另一篇A类paper了，涉及的数学和图论功底可能不是我能掌握。但是我觉得如果能想办法结合CPU和内存来放进edge的weight里一起算，应该会得到更好的结果。&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Leon</dc:creator><pubDate>Sat, 06 Apr 2013 00:28:00 +0800</pubDate><guid>tag:www.liustrive.com,2013-04-06:pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html</guid><category>notes</category><category>cloud</category><category>distribution</category></item><item><title>Yarn初探（一）资源调度</title><link>http://www.liustrive.com/pages/2013/03/29/yarnchu-tan-zi-yuan-diao-du.html</link><description>&lt;h3&gt;资源调度器&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://i.imgur.com/1pY6g1v.jpg" /&gt;
YARN的资源管理器本质上像一个事件处理器，需要处理6种SchedulerEvent类型的事件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NODE_REMOVED&lt;/strong&gt;：表示集群中被移除一个计算节点，资源调度器收到该事件时需要从可分配资源总量中移除相应的资源量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NODE_ADDED&lt;/strong&gt;：表示集群中增加了一个计算节点，资源调度器收到该事件时需要将新增的资源量添加到可分配资源总量中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;APPLICATION_ADDED&lt;/strong&gt;：表示ResourceManager收到一个新的Application。资源管理器需将该Application添加到相应的数据结构中。通常，资源管理器需要为每个application维护一个独立的数据结构，以便于统一管理和资源分配。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;APPLICATION_REMOVED&lt;/strong&gt;：事件APPLICATION_REMOVED表示一个Application运行结束，资源管理器需将该Application从相应的数据结构中清除。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CONTAINER_EXPIRED&lt;/strong&gt;：当资源调度器将一个container分配给某个ApplicationMaster后，如果该ApplicationMaster在一定时间间隔内没有使用该container，则资源调度器会对该container进行再分配。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NODE_UPDATE&lt;/strong&gt;：NodeManager通过心跳机制向ResourceManager汇报各个container运行情况，会触发一个NODE_UDDATE事件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中，NODE_UPDATE事件发生时表示可能有新的container得到释放，因此该事件会触发资源分配，该事件是6个事件中最重要的事件，它会触发资源调度器最核心的资源分配机制。&lt;/p&gt;
&lt;h3&gt;资源管理与分配&lt;/h3&gt;
&lt;p&gt;相比于MapReduce v1中的资源调度器，YARN采用了事件驱动的模型，因此编写起来更加复杂。同MapReduce v1一样，YARN也自带了三种常用的调度器，分别是FIFO，Capacity Scheduler和Fair Scheduler，其中，第一个是默认的调度器，它属于批处理调度器，而后两个属于多租户调度器，采用树形多队列的形式组织资源。&lt;/p&gt;
&lt;p&gt;YARN的资源表示模型目前只支持CPU和内存的管理与分配，NodeManager启动时，会向ResourceManager注册，而注册信息中会包含该节点可分配的CPU和内存总量，YARN为了更细化的分配CPU资源，将物理CPU划分为多个虚拟CPU，用户提交应用程序时，可以指定每个任务需要的虚拟CPU个数。而对内存资源，YARN采用进程监控的方式控制内存，每个NodeManager会启动一个额外监控线程监控每个container内存资源使用量，一旦发现它超过约定的资源量，则会将其杀死。&lt;/p&gt;
&lt;p&gt;YARN的资源分配模型是以队列的形式组织的，每个用户可属于一个或多个队列，且只能向这些队列中提交application。每个队列被划分了一定比例的资源。而资源分配过程是异步的，资源调度器将资源分配给一个application后，不会立刻push给对应的ApplicaitonMaster，而是暂时放到一个缓冲区中，等待ApplicationMaster通过周期性的RPC函数主动来取，即采用了pull-based模型，而不是push-based模型。&lt;/p&gt;
&lt;h3&gt;有关ApplictionMaster&lt;/h3&gt;
&lt;p&gt;ApplicationManager的三大模块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AMLivelinessMonitor&lt;/strong&gt;：周期性遍历所有ApplicationMaster，如果一个ApplicationMaster在一定时间内未汇报心跳信息，则认为它死掉了，它上面所有正在运行的Container将被置为运行失败，AM本身会被重新分配到另外一个节点上执行。&lt;/p&gt;
&lt;p&gt;对于运行失败的Container, RM不会重新执行，它只会通过心跳机制告诉对应的AM，由AM决定是否重新执行，如果需要，则AM重新向RM申请资源。
- &lt;strong&gt;ApplicationMasterLauncher&lt;/strong&gt;：处理AMLauncherEvent类型的事件，该类型事件有两种，分别是请求启动一个AM的“LAUNCH”和请求清理一个AM的“CLEANUP”。ApplicationMasterLauncher维护了一个线程池，从而能够尽快地处理这两种事件：
- 收到了“LAUNCH”类型的事件，它会与对应的NodeManager通信，要求它启动ApplicationMaster。具体步骤如下：
    1. 创建一个ContainerManager协议的Client；
    2. 向对应的NodeManager发起连接请求；
    3. 启动AM所需的各种信息，并封装成一个StartContainerRequest对象；
    4. 通过RPC函数ContainerManager. startContainer()发送给对应的NM。
- 收到了“CLEANUP”类型的事件，它会与对应的NodeManager通信，要求它杀死ApplicationMaster。整个过程与启动AM的过程类似。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ApplicationMasterService&lt;/strong&gt;：处理来自ApplicationMaster的请求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注册&lt;/li&gt;
&lt;li&gt;心跳&lt;/li&gt;
&lt;li&gt;清理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中，注册是ApplicationMaster启动时发生的行为，请求包中包含AM所在节点，RPC端口号和tracking URL等信息；心跳是周期性行为，包含请求资源的类型描述、待释放的Container列表等，而AMS则为之返回新分配的Container、失败的Container等信息；清理是应用程序运行结束时发生的行为，ApplicationMaster向RM发送清理应用程序的请求，以回收资源和清理各种数据结构。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Leon</dc:creator><pubDate>Fri, 29 Mar 2013 23:45:00 +0800</pubDate><guid>tag:www.liustrive.com,2013-03-29:pages/2013/03/29/yarnchu-tan-zi-yuan-diao-du.html</guid><category>Yarn</category></item></channel></rss>