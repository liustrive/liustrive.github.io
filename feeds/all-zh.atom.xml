<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>古董Leon</title><link href="http://www.liustrive.com/" rel="alternate"></link><link href="http://www.liustrive.com/feeds%5Call-zh.atom.xml" rel="self"></link><id>http://www.liustrive.com/</id><updated>2014-07-26T12:12:00+08:00</updated><entry><title>Distributed Representations of Words and Phrases and their Compositionality 阅读笔记</title><link href="http://www.liustrive.com/pages/2014/07/26/distributed-representations-of-words-and-phrases-and-their-compositionality-yue-du-bi-ji.html" rel="alternate"></link><updated>2014-07-26T12:12:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2014-07-26:pages/2014/07/26/distributed-representations-of-words-and-phrases-and-their-compositionality-yue-du-bi-ji.html</id><summary type="html">&lt;h6&gt;via: &lt;a href="www.liustrive.com"&gt;liustrive.com&lt;/a&gt;&lt;/h6&gt;
&lt;p&gt;文章提出了一些对Skp-gram模型的扩展，提出一种NCE（Noise Contrastive Estimation ）方法用以代替上文hierarchical softmax方法，从而获得更高的效率和对高频词的更好的词向量结果。
文中提到词向量表示的一大弊端是它们无法表示一些词组成的常用短语，这短语不是字面上把其组成词连接起来的意义，于是提出扩展，将基于词的模型变成基于短语的模型，扩展的原理是先抓出这些短语，然后把这些短语作为整体的“词”加入训练集，然后使用上文实验部分中提到的方法，利用词向量代数运算验证准确性。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;The Skip-gram Model&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;上文介绍过skip-gram模型的结构，给定一句由训练词{w1,w2,w3…wT}，其目标是最大化平均对数概率，c为目标词的前后上下文长度
 $$\frac{1}{T}\sum_{t=1}^T \sum_{-c\leq j \leq c, j\neq0} \log p(w_{t+j}|w_t)$$
其中，$p(w_{t+j} | w_t)$ 用softmax函数计算：
 $$\hat{p}(w_t|w_{t-1},...,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}$$&lt;br /&gt;
即：
 $$p(w_O| w_I) = \frac{exp({v'}&lt;em w_I="w_I"&gt;{w_O} {}^{\mathrm{T}} v&lt;/em&gt;)}{\sum_{w=1}^W exp(v'&lt;em w_I="w_I"&gt;w {}^{\mathrm{T}}v&lt;/em&gt;)}$$  &lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Hierarchical Softmax&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Hierarchical Softmax是full softmax的一种计算量优化，其目标是减少计算：&lt;br /&gt;
$$p(A|C) = \frac{e^{-E(A,C)}}{\sum_{v=1}^V e^{-E(wv,C)}}$$
时所需的巨大计算量，使其运算量从计算|V|个输出节点变成log|V|
其推导过程如下：&lt;br /&gt;
&lt;center&gt;&lt;img alt="" src="http://cl.ly/YFsO/cbow-exp_2_1.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Negative Sampling&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;文章提出一种代替hierarchical softmax的方法（NCE），NCE能够求出softmax的对数概率函数近似最大，由于skip-gram模型只关心词向量的质量，所以在保证词向量准确性的条件下，还可以进一步简化NCE，文章定义了NCG方法：
每个$\log p(W_O | W_I) $的运算被下式替代：&lt;br /&gt;
$$\log \sigma (v'&lt;em w_I="w_I"&gt;{w_O} {}^{\mathrm{T}}v&lt;/em&gt;) + \sum_{i=1}^{k} E_{w_i \sim P_n(w)}\log \sigma(-v'&lt;em w_I="w_I"&gt;{w_i} {}^{\mathrm{T}}v&lt;/em&gt;)]$$&lt;br /&gt;
其中k是为每个数据样本选用k个负类样本，在小的数据集时可以取5-20，大的数据集甚至取2-5就够用，Negative sampling 和NCE的主要区别是NCE同时需要样本和噪声分布的数值概率 ？  &lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Subsampling of Frequent Words&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;在大型的语料中，一些词会有非常高的词频，比如the, a , in 这种类型词，但是这种高频词往往比一些低频词提供更少的有效信息，所以为了逆转这种不平衡性，文章提出一种二次抽样方法：&lt;br /&gt;
 $$P(w_i) = 1-\sqrt{\frac{t}{f(w_i)}}$$
其中$f(w_i)$是词$w_i$的词频，t是预设的阈值，词频高于t的$w_i$ 具有$P(w_i)$的概率被抛弃。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;实验部分：&lt;/strong&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;strong&gt;传统的实验&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;这篇文章的实验任务也采用上文的代数运算准确率衡量，实验描述参见上文Task description 
文中实验Skip-gram采用10亿数据量的数据集，抛弃词频少于5次的词，实验结果如图Table1
 &lt;center&gt;&lt;img alt="" src="http://cl.ly/YFvs/cbow-exp_2_2.jpg" /&gt;&lt;/center&gt;  &lt;/p&gt;
&lt;p&gt;可以观察到Negative Sampling准确率高于HS算法，比原版的NCE也有少许提升，同时观察到以$10^{-5}$为阈值二次采样过滤后，大幅缩短了训练时间，同时少量提升了词向量准确度。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;短语实验&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;上文提到过，一些由词组成的固定短语并不是其字面意义，于是作者抓出这些固定短句，并以其为整体加入词库而不是拆分成单个的词，抓出这些词的具体方法是：&lt;br /&gt;
$$score(w_i,w_j) = \frac{count(w_i,w_j)-\delta}{count(w_i)\times count(w_j)}$$
基本上就是词以固定形式出现的次数减去一个折扣系数与他们各自出现频率的比值，已决定这个短语的权值，最后权值超过预设的阈值的短语被取出作为固定短语，文中实验采用递降的阈值对训练集跑了2-4次这样的取短语操作，使长固定短语也能被抓出&lt;br /&gt;
&lt;center&gt;&lt;img alt="" src="http://cl.ly/YGDD/cbow-exp_2_3.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这里具体怎么个2-4次法，前次的输出作为后次的数据集还是什么，文中没说清。&lt;/p&gt;
&lt;p&gt;而后作者设计了针对固定短语的实验，与前文代数运算实验相近，只是测试用例变成：
组成固定短语的词 →固定短语
的关系，采用的样例如Table2：&lt;br /&gt;
&lt;center&gt;&lt;img alt="" src="http://cl.ly/YF8x/cbow-exp_2_4.jpg" /&gt;&lt;/center&gt;
根据此类测试用例，实验这对固定短语的词向量代数运算，数据集仍然采用10亿词量大小的Google News，并向词库加入抓取出的固定短语，实验结果发现，HS经过subsampling后具有最好的准确性，这表明subsampling不但能提升训练速度，也能提升一些情况下的准确率。
为了最大化这个固定短语词向量的准确性，另外的实验采用了1000维n=全句的HS算法，并采用330亿词数的数据集，模型准确率达到了72%，而数据集降到6B时，准确率为66%，说明训练数据的规模对词向量的准确率是决定性的。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Additive Compositionality&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;文章最后还讨论了词向量的加法语义合成，如Table4所示：&lt;br /&gt;
 &lt;center&gt;&lt;img alt="" src="http://cl.ly/YGCO/cbow-exp_2_5.jpg" /&gt;&lt;/center&gt;&lt;br /&gt;
词向量加法能得到与相加的两词语义最接近的词或短语，这主要是因为在训练过程中，相加的两词有更大可能与结果短语或词在同一句中，比如“中国”+“首都”= “北京”&lt;/p&gt;
&lt;p&gt;文章最后横向与相近工作的其它模型做了比较，比较训练时间以及词向量的准确率，结果表明Skip-gram phrase model 具有最快的训练时间（随着数据集规模的增长只有少量时间增加）和很好的准确性。&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="NLP"></category><category term="word2vec"></category><category term="Mikolov"></category></entry><entry><title>Efﬁcient Estimation of Word Representations in Vector Space 阅读笔记</title><link href="http://www.liustrive.com/pages/2014/07/25/efficient-estimation-of-word-representations-in-vector-space-yue-du-bi-ji.html" rel="alternate"></link><updated>2014-07-25T15:51:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2014-07-25:pages/2014/07/25/efficient-estimation-of-word-representations-in-vector-space-yue-du-bi-ji.html</id><summary type="html">&lt;h6&gt;via: &lt;a href="www.liustrive.com"&gt;liustrive.com&lt;/a&gt;&lt;/h6&gt;
&lt;h4&gt;&lt;strong&gt;背景&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;现有模型，比如前文Bengio的采用非线性激活函数tanh的神经网络模型，虽然准确度上有优势，但是算法的复杂度较高，无法训练更大的数据集，比如特征向量维度为50-100情况下训练超过几百兆个的语料。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;目标&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;提出一种模型使其能够训练百万级词表下数十亿词量的训练集，并产出优质词特征向量。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Model Architectures &lt;/strong&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;strong&gt;Feedforward Neural Net Language Model (NNLM)&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;此即前文所述前向神经网络语言模型，采用四层神经网络（原文讲实际是三层神经网络）
输入层、投影层、隐藏层、输出层
每个训练样本的计算复杂度为：
$$Q = N × D + N × D × H + H × V$$
其中，N为n-gram模型中n值，D为词向量维度，H是隐藏层size（通常500-1000），V是词表大小，同时也是输出层的节点数量，N&lt;em&gt;D为输入层到投影层权重个数，H&lt;/em&gt;V表示隐藏层到输出层的权重个数，N&lt;em&gt;D&lt;/em&gt;H是投影层到隐藏层权重个数。&lt;br /&gt;
在文中的模型中，使用Huffman树来表示输出层hierarchical softmax，从而输出层参数从$H&lt;em&gt;V\rightarrow H&lt;/em&gt;log(V)$。&lt;br /&gt;
但是这个提速并不能使神经网络语言模型获得决定性提升，因为计算瓶颈变成了N&lt;em&gt;D&lt;/em&gt;H，于是提出了没有隐藏层的模型结构。&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;recurrent neural net language model（RNNLM）&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;递归神经网络语言模型没有投影层，只有输入层、隐藏层、输出层，该模型使用延时连接将隐藏层相连接，以使模型具有短期记忆，训练样本计算复杂度为：
$Q = H × H + H × V$
其中词向量与隐藏层维度都是H，这种模型也可以通过Huffman树表示输出层的方式提升效率&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;New Log-linear Models&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;前文提到，隐藏层计算复杂度成为性能瓶颈，作者观察到，隐藏层的非线性结构导致了需要大量的计算，虽然这种非线性结构更加精确，但是在这一节中作者介绍了两种模型，去除了隐藏层，并且分两步训练：1、计算词特征向量，2、在词特征向量基础上训练n-gram神经网络，以此大幅提升效率。&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Continuous Bag-of-Words Model&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="http://cl.ly/YFTh/CBOW.jpg" /&gt;  &lt;/center&gt;&lt;/p&gt;
&lt;p&gt;CBOW模型与前向神经网络模型相近，但是去除了隐藏层，所有词都通过加和平均投影到同一个D维向量上，训练的复杂度为：
$$Q = N × D + D × \log_2(V)$$&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Continuous Skip-gram Model&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="http://cl.ly/YF8I/skip_gram.jpg" /&gt;&lt;/center&gt;
Skip-gram模型与CBOW模型相似，只不过后者根据上下文预测当前词，而前者根据当前词预测上下文可能存在的词，训练的计算复杂度为：
$$Q = C × (D + D × \log_2(V ))$$
其中C是词间最大距离&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;实验部分&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;作者表示中国相似法国相似意大利这种例子太没意思，前文也提到了训练产出的词向量具有（一定程度的）代数运算性质，于是文中设计了一系列实验，用代数运算的结果来验证模型的训练结果能否反映词之间的关系，并以此表示词向量的好坏。&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Task description&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="http://cl.ly/YFSt/cbow-exp.jpg" /&gt;
作者采用的测试分类&lt;/center&gt;
作者设计了五类语义问题以及九类语法问题，比如男-女类 兄弟→姐妹 孙子→孙女，于是应该得到vector(“兄弟”) – vector(“姐妹”) + vector(“孙子”) = vector(“孙女”)
通过验证这些类别中的词组是否得到正确的代数运算结果，衡量词向量的准确性&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Maximization of Accuracy&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;本文实验采用的是Google News语料训练词向量，文中为了衡量模型什么样的输入和参数下最快时间内得到最好的训练产出，先采用一半的语料与一半的向量维度，再依次同步增加，结果如Table2
 &lt;center&gt;&lt;img alt="" src="http://cl.ly/YG4E/cbow-exp2.jpg" /&gt;&lt;/center&gt;
可以通过结果观察到各在一定范围内，增加向量维度和增大训练集都能提升准确率，文中使用的学习速率为0.025.&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Comparison of Model Architectures&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;模型间在相同维度和训练集大小情况下的比较结果，既采用了前文的代数运算方式统计准确性，也引入了通用的词间语法相似性的统计结果来反映词向量准确性。
结果如Table4所示：&lt;br /&gt;
 &lt;center&gt;&lt;img alt="" src="http://cl.ly/YFJW/cbow-exp3.jpg" /&gt;&lt;/center&gt;&lt;br /&gt;
RNNLM效果最差，虽然NNLM的非线性隐藏层理论上提供更高的精确性，但是由于运算量限制了词向量维度和训练集大小，NNLM的效果仍然不如CBOW和Skip-gram（排名分先后）
另外根据Table5的结果&lt;br /&gt;
&lt;center&gt;&lt;img alt="" src="http://cl.ly/YFkJ/cbow-exp4.jpg" /&gt;&lt;/center&gt;
3次迭代会提升准确度，但是影响不如其它因素那么大。  &lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Large Scale Parallel Training of Models&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;前文提到过，模型提出的目的就是能更快速的训练更大的语料，作者在分布式平台上实现了文中模型，用Google News的6B数据集做输入，实验结果如Table6：&lt;br /&gt;
&lt;center&gt;&lt;img alt="" src="http://cl.ly/YF2d/cbow-exp5.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;可以发现CBOW和Skip-gram首先在速度上有绝对优势，NNLM甚至无法计算1000维词向量，准确性上也超过了NNLM模型。&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Microsoft Research Sentence Completion Challenge&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;作者最后引入微软的句子补完测试，其内容是1040个句子，去掉其中一个词，提供五个词作为候选，目标是找出最合适的词使句子完整，文中使用的skip-gram模型测试，其准确率并不如LSA模型，但是结合RNNLM，通过加权平均，能使结果准确率提升到历史最高的58.9%。&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="NLP"></category><category term="word2vec"></category><category term="Mikolov"></category></entry><entry><title>A Neural Probabilistic Language Model 入门笔记</title><link href="http://www.liustrive.com/pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html" rel="alternate"></link><updated>2014-07-12T00:29:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2014-07-12:pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html</id><summary type="html">&lt;h5&gt;Via:  &lt;a href="www.liustrive.com"&gt;liusrive.com&lt;/a&gt;&lt;/h5&gt;
&lt;hr /&gt;
&lt;h2&gt;&lt;strong&gt;目标：&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;learn the joint probability function of sequences of words in a language&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;学习某种语言中单词序列的联合概率函数
即根据训练集按照一定的算法建立一种数学模型，使之能够计算出某个句子在该模型下出现的概率，以及根据前N-1个词决定第N个词出现的概率。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注：见背景知识：语言模型及n-gram 模型&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;该目标面临的主要困难：&lt;/strong&gt;&lt;br /&gt;
Curse of dimensionality 维数灾难&lt;br /&gt;
具体表现为：&lt;br /&gt;
&lt;em&gt;a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training&lt;/em&gt;&lt;br /&gt;
被测试的词经常并未出现在训练集中，对应概率是0，模型无法准确处理，即模型的泛化能力低&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;传统的解决办法：&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;concatenating very short overlapping sequences seen in the training set&lt;/em&gt;&lt;br /&gt;
采用n-gram模型，文中提到了两种常用方法：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;回退法（back-off trigram models)&lt;br /&gt;
n-gram 值为0，则采用n-1 gram并乘以一个权重模拟  &lt;/li&gt;
&lt;li&gt;平滑法（smoothed trigram models）&lt;br /&gt;
有多种平滑方法，比较基础的是为未出现的词或n元组赋一个非零值，并为出现过的词或n元组递增同样的值，比如为unseen的n-gram赋1，并将所有seen n-gram加1参见&lt;a href="http://en.wikipedia.org/wiki/N-gram#Smoothing_techniques"&gt;wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;注：参见背景知识：基本平滑算法&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;以上方法存在一些问题：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;it is not taking into account contexts farther than 1 or 2 words (截止文章发表时)&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;it is not taking into account the “similarity” between words&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;传统的方法不能建立更长的关系，无法训练出高阶语言模型；
同时，并未考虑词的相似性，相似的词往往可以出现在相同的句式，比如：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The cat is walking in the bedroom
A dog was running in a room&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;本文提出的方法：&lt;/strong&gt;&lt;br /&gt;
&lt;img alt="文中方法" src="http://cl.ly/YFx2/NN_paper_pic1.jpg" /&gt;&lt;br /&gt;
将词典中的词用词特征向量表示，通过这些特征向量来表示次序列的联合概率模型，并在模型训练阶段通过最大化训练数据的对数似然函数同时训练好特征向量和概率模型的参数。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;A Neural Model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;训练集:
sequence w1…wT ，其中的词wt 属于于很大的有限集词表V
目标模型： 
$$f(w_t,...,w_{t=n+1}) = \hat{P}(w_t | w_1^{t-1})$$
将目标分解为两部分：
&lt;center&gt; &lt;img alt="" src="http://cl.ly/YFku/NNLM.jpg" /&gt;&lt;br /&gt;
图1 三层神经网络&lt;/center&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将词表中的元素i映射为distributed feature vectors，即特征向量C(i)，C是一个|V|*m的矩阵，m为特征向量维数，C(i)为C中一行;&lt;/li&gt;
&lt;li&gt;根据上下文生成的概率分布函数g，其输入为前n词的 (C(wt−n+1),••• ,C(wt−1))，输出为一个向量，其第i的元素表示 $\hat{P}(w_t = i|w_1^{t-1})$
$$f(i,w_{t-1},...,w_{t-1-n+1} = g(i,C(w_{t-1},...,C(w_{t-n+1})))$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;函数f由两个映射关系g和C构成，C的参数就是|V|&lt;em&gt;m矩阵表示的词特征向量，w到C(w)的映射表示从矩阵C取出一行，C和g的参数 $θ= (C,ω)$ 构成模型需要学习的参数集，训练的过程是寻找能够使训练语料的penalized log-likelihood最大的$θ$，学习过程中参数数量和词表大小、上下文数量n成线性关系。
 $$L=\frac{1}{T}\sum_t log f(w_t,w_{t-1},...,w_{t-n+1};\theta)+ R(\theta)$$
文中使用的实际包含一个隐藏层的神经网络进行训练，输出层共|V|个节点, 采用softmax激活函数将y归一化成概率公式：
 $$\hat{P}(w_t|w_{t-1},...,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}$$
&lt;/em&gt;注：此处公式参见背景知识：神经网络*&lt;/p&gt;
&lt;p&gt;其中，$y_i$ 为下一个词为i的未归一化log概率，隐藏层使用d+Hx得到d为偏置项，具有参数b,W,U,d,H： 
 $$y=b+Wx+U tanh(d+Hx)$$
输入层x为将C(wt−1),C(wt−2),••• ,C(wt−n+1)这些向量首尾相连形成的（n-1）m维向量：
 $$x=(C(w_{t-1}),C(w_{t-2}),...,C(w_{t-n+1}))$$
于是需要训练的参数集合为：
 $$\theta = (b,d,W,U,H,C)$$
其中，U为一个|V|&lt;em&gt;h矩阵，表示隐藏层到输出层的参数，矩阵W为一个|V|&lt;/em&gt;（n-1）m的矩阵，包含了输入层到输出层的直连边，（参见图1中虚线以及正文末算法）。&lt;/p&gt;
&lt;p&gt;最后通过随机梯度下降法(SGD)优化模型，最终获得词向量和语言模型。
 $$\theta \leftarrow \theta +  \varepsilon\frac{\partial log\hat{P}(w_t|w_{t-1},...,w_{t-n+1})}{\partial \theta}$$&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Parallel Implementation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;虽然前文提到本文模型参数增长为线性增长，但由于采用n-gram模型时，计算P(wt|wt−1,...,wt−n+1) 时并不需要计算词表中每一个词的概率，神经网络模型所需的计算量远大于n-gram模型。
有关并行处理，在共享内存多处理机上文中算法实现首先采用每个处理器处理一部分子数据集，update模型的参数时需要等待资源锁，结果导致大部分CPU周期用来等待其它处理器释放参数的写锁；文章随后采用了异步实现，任何处理器任何时间都可以写共享内存，这样的实现会导致一些处理器update的参数被其它处理器写线程覆盖，但是实验表明这种noise不会降低训练速度。
文章最终使用fast network cluster作为环境，采用了parallelize across the parameters 的方式进行计算，每个CPU负责计算输出的部分子集的unnormalized probability，并且更新涉及到的输出单元参数。&lt;/p&gt;
&lt;p&gt;这种策略下，CPU间的通讯只需要包括:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;output softmax的归一化因子&lt;/li&gt;
&lt;li&gt;隐藏层和词特征层的梯度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于单一训练样本，假设有词表数|V|，隐藏单元h，词序n，词特征项m，则计算复杂度为： |V|(1+nm+h)+h(1+nm)+nm
单一训练样本复杂度除以计算weighted sums of output units复杂度得到：
 $$\frac{|V|(1+(n-1)m+h)}{|V|(1+(n-1)m+h)+h(1+(n-1)m)+(n-1)m}$$
可知总体上将输出单元计算并行化是有优势的。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;BP part&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Forward phase，前向计算：&lt;/strong&gt;&lt;br /&gt;
1. 为词特征层前向计算  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$x(k) \leftarrow C(w_{t-k})$&lt;br /&gt;
  $x=(x(1),x(2),...,x(n-1))$  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2. 为隐藏层前向计算，d是偏置项，激活函数tanh  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$o \leftarrow d + Hx$&lt;br /&gt;
   $a \leftarrow tanh(o)$  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;3. 为第i个块内输出做前向计算  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$s_i \leftarrow 0$&lt;br /&gt;
   for each j in i-th block&lt;br /&gt;
  　$y_j \leftarrow b_j + a.U_j$&lt;br /&gt;
  　if( direct connections) &lt;br /&gt;
  　　$y_j \leftarrow y_j + x.W_j$  &lt;br /&gt;
  　$p_j \leftarrow e^{y_j}$&lt;br /&gt;
  　$S_i \leftarrow S_i + p_j$   &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$S=∑_i s_i$，此计算需在所有处理器间共享$S_i$&lt;/p&gt;
&lt;p&gt;4. 正规化概率&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对第i块内输出以j为下标循环&lt;br /&gt;
$p_j \leftarrow p_j/S$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;5. 更新对数似然函数&lt;/p&gt;
&lt;p&gt;$$$$&lt;strong&gt;Backward/update phase 后向计算/更新阶段&lt;/strong&gt;&lt;br /&gt;
1. 清空梯度向量∂L/∂a，∂L/∂x&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对第i个块内以j为下标循环&lt;br /&gt;
  　$\frac{\partial L}{\partial y_j} \leftarrow 1_{j==w_t - p_j}$&lt;br /&gt;
 　$b_j \leftarrow b_j + \varepsilon \frac{\partial L}{\partial y_j}$&lt;br /&gt;
 　If(direct connections)&lt;br /&gt;
 　　$ \frac{\partial L}{\partial x} \leftarrow \frac{\partial L}{\partial x} + \frac{\partial L}{\partial y_j}W_j$&lt;br /&gt;
　$ \frac{\partial L}{\partial a} \leftarrow \frac{\partial L}{\partial a} + \frac{\partial L}{\partial y_j}W_j$&lt;br /&gt;
　If(direct connection)&lt;br /&gt;
　　$W_j \leftarrow W_j + \varepsilon\frac{\partial L}{\partial y_j}x$&lt;br /&gt;
　$U_j \leftarrow U_j + \varepsilon\frac{\partial L}{\partial y_j}x$  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2. 对$\frac{\partial L}{\partial a}, \frac{\partial L}{\partial x}$求和并在所有处理器上共享&lt;br /&gt;
3. 后向传播并更新隐藏层权重  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;k由1到h&lt;br /&gt;
　$\frac{\partial L}{\partial o_k} \leftarrow (1 -a_k^2)\frac{\partial L}{\partial a_k}$&lt;br /&gt;
　$\frac{\partial L}{\partial x} \leftarrow \frac{\partial L}{\partial x} + H'\frac{\partial L}{\partial o}$&lt;br /&gt;
　$d \leftarrow d + \varepsilon \frac{\partial L }{\partial o}$&lt;br /&gt;
　$H \leftarrow H + \varepsilon \frac{\partial L }{\partial o}x'$  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;4. 为输入更新此特征向量  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$K ∈ (1,n-1)$&lt;br /&gt;
$C(w_{t-k} \leftarrow C(w_{t-k}) + \varepsilon \frac{\partial L}{\partial x(k)})$&lt;br /&gt;
 其中$∂L/(∂x(k))$是∂$L/∂x$的第k分量&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;背景知识&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;strong&gt;语言模型及n-gram 模型&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;语言模型：&lt;/strong&gt;&lt;br /&gt;
统计式的语言模型是借由一个机率分布，而指派机率给字词所组成的字串：
 $$P(w_1,...,w_m)$$
由此得到推论：
$$P(w_1,w_2,…,w_t)=P(w_1)×P(w_2|w_1)×P(w_3|w_1,w_2)×…×P(w_t|w_1,w_2,…,w_{t−1})$$
简单的讲就是看一串字词的组合有多大可能出现，换句话讲就是有没有可能是正常人类语言。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N-gram模型：&lt;/strong&gt;&lt;br /&gt;
语言的一种统计学模型为，根据已出现的词，得出下一个词出现的概率，即：
 $$\hat{p}(w_1^T)\prod_{t=1}^T \hat{p}(w_t|w_1^{t-1})$$
在建立自然语言的统计学模型时，为简化问题，假设第t个词只与其前n-1个词有关，于是有：
 $$\hat{p}(w_t|w_1^{t-1})\approx \hat{p}(w_t|w_{t-n+1}^{t-1})$$
根据这个模型，通过对语料建立一个多项分布模型，采用最大似然估计，可以获得任意句子在该模型下出现的概率。
 $$p(w_i|w_1w_2...w_{i-1}) = \frac{C(w_1w_2...w_{i-1}w_i)}{C(w_1w_2...w_{i-1})}$$&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;基本平滑算法&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;回退Back-off&lt;/strong&gt;&lt;br /&gt;
    如果n-gram的值为零，则用n-1 gram来计算  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;平滑Smoothing&lt;/strong&gt;&lt;br /&gt;
    将MLE方法与其它方向相混合，保证没有0概率的值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加1平滑&lt;/strong&gt;&lt;br /&gt;
    T:训练数据,V:词表,w: 词
          预测 p’(w|h)=(c(h,w)+1)/(c(h)+|V|)
          特别:非条件分布时p’(w)=(c(w)+1)/(|T|+|V|)
    问题：经常会|V|&amp;gt;c(h)，甚至|V|&amp;gt;&amp;gt;c(h) &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小于1平滑：&lt;/strong&gt;&lt;br /&gt;
    加入λ系数 
    -T:训练数据,V:词表,w: 词
       预测 p’(w|h)=(c(h,w)+λ)/(c(h)+ λ |V|), λ&amp;lt;1
        特别:非条件分布时p’(w)=(c(w)+ λ)/(|T|+ λ |V|)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Good-Turing:&lt;/strong&gt;&lt;br /&gt;
    1、适用于评估大规模的数据
    2、相似点：
    $pr(w)=(c(w)+1)&lt;em&gt;N(c(w)+1)/(|T|&lt;/em&gt;N(c(w)))$,
    其中：N(c)是数目为c的词的数量
    特别：对于训练集中没有出现的词，$c(w)=0 pr(w)=N(1)/(|T|*N(0))$
    有利于数量少的词语（&amp;lt;5-10, 但N(c)较大）
    “调富济贫”
    归一化（可以得到$\lambda_i &amp;gt; 0, \sum_{i=0...n} \lambda_i = 1$）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;典型n-gram语言模型的平滑:&lt;/strong&gt; &lt;br /&gt;
 1、采用$λ=(λ0,λ1,λ2,λ3)$ &lt;br&gt; 
 2、归一化：
       $λi &amp;gt; 0, ∑_{(i=0…n)}λ_i=1 $&lt;br&gt;
3、极大似然估计
    : 固定p3,p2,p1和|V|，根据训练数据确定参数
    : 再寻找一组{λi}使得交叉熵达到最小
    （使数据的概率达到最大）：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;EM平滑算法:&lt;/strong&gt;&lt;br /&gt;
1、从某些λ开始，如λj&amp;gt;0,所有j∈0..3&lt;br /&gt;
2、计算每个λj的期望数值 &lt;br /&gt;
3、采用“Next λ”公式计算的λj新集合 &lt;br /&gt;
4、返回2，除非遇到终止条件 &lt;br /&gt;
    终止条件为： λ收敛 
      简单设定一个ε，当step3中对每个j都有 
      $|λj-λj ,next|&amp;lt; ε$时终止 
 &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;神经网络的本质是两个阶段非线性统计模型：
&lt;center&gt; &lt;img alt="" src="http://cl.ly/YG5b/NN_note_basic.jpg" /&gt;
图2神经网络模型&lt;/center&gt;
如图2所示，中间红色Z为导出特征，也称隐藏层，有输入的线性组合创建Z，再用Z的线性组合建立以Y为目标的模型。
 $$Z_m = \sigma(\alpha_{0m} + \alpha_m^TX),m=1,...,M$$
 $$T_k = \beta_{0k} + \beta_k^TZ, k=1,...,K$$
 $$f_k(X) = g_k(T), k=1,...,K$$
激活函数$σ（ע）=exp(y)/(1+exp(y))=1/(1+exp(-y))$ 
输出函数g_k (T)是对向量T的最终变换，早期K分类采用恒等函数，后来被softmax函数取代，因其可以产生和为1的正估计：
 $$g_k(T) = \frac{e^{T_k}}{\sum_{\epsilon=1}^{K}e^{T_\epsilon}}$$&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Training of Neural Networks&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;未知参数称为权，用$\theta$表示权的全集 
对于回归和分类问题，我们分别使用误差的平方和，平方误差或互熵（离散）作为拟合的度量 
 $${\alpha_{0m},\alpha_m;m=1,2,...,M} M(p+1) weights$$
 $${\beta_{0k},\beta_k;k=1,2,...,K} K(M+1) weights$$
 $$R(\theta) = \sum_{k=1}^K \sum_{i=1}^{N}(y_{ik}-f_k(x_i))^2$$
  $$R(\theta) = - \sum_{i=1}^N \sum_{k=1}^{K}(y_{ik}-\log f_k(x_i))$$&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="NLP"></category><category term="Neural Network"></category><category term="Bengio"></category></entry><entry><title>Corona 笔记</title><link href="http://www.liustrive.com/pages/2013/05/12/corona-bi-ji.html" rel="alternate"></link><updated>2013-05-12T12:19:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2013-05-12:pages/2013/05/12/corona-bi-ji.html</id><summary type="html">&lt;h3&gt;Corona 初探&lt;/h3&gt;
&lt;p&gt;本周关注了一下另一种号称下一代MapReduce的框架：&lt;strong&gt;Corona&lt;/strong&gt;。它是facebook的开源框架，代码在：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona"&gt;https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Cornna的设计需求与Apache的YARN框架十分相似，它的发布文档：&lt;/p&gt;
&lt;p&gt;“&lt;a href="http://www.jorditorres.org/wp-content/uploads/2012/11/CC-MEI-Corona-AntonioRodriguez.pdf"&gt;Under the Hood: Scheduling MapReduce jobs more efficiently with Corona&lt;/a&gt;”&lt;/p&gt;
&lt;p&gt;其中提到，当前Map-Reduce所使用的single Job Tracker在Facebook的运行环境下已经遇到瓶颈，在Hadoop Corona中，集群的资源是由central Cluster Manager统一分配调度，每个job拥有各自单独的Corona Job Tracker，这些与YARN框架的设计思路非常相近，解决了Map-Reduce中存在的扩展性、单点故障等不足。&lt;/p&gt;
&lt;p&gt;相比于YARN，Corona似乎更贴近Map-Reduce的原版，其代码在原版Map-Reduce基础上修改而来，似乎更贴近原有Map-Reduce的用户；另一大优势在于Corona所采用的Push-based通信机制，在原版Map-Reduce和YARN中，采用轮询方式使client获取信息，比如心跳机制，当规模过大后，会产生一定延迟，而Corona采用Push-based通信机制，server直接把信息推送至client，为实现这一通信机制，Corona似乎采用了更多的通信协议（待考证）。&lt;/p&gt;
&lt;p&gt;Corona相比与Yarn的缺点也是由于它更贴近原版所产生，比如Corona不能提供Yarn的用户设置所需内存、CPU等特性；任务调度中仍采用slot作为资源分配元，资源利用率不如Yarn；Corona也不支持Yarn那种灵活的多计算框架支持，因为它的TaskTracker还是执行Map和Reduce task。
总的来说，Corona更像是对原版Map-Reduce的升级，从代码量上也可见一斑，Yarn的&lt;a href="http://www.fightrice.com/mirrors/apache//hadoop/core/"&gt;代码&lt;/a&gt; 十五万行代码完全重构，与原版Map-Reduce没有任何复用，而Corona相对原版有大量的重复，更接近与改进了部分框架的升级。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corona说明书上宣称的特点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better scalability and cluster utilization&lt;/li&gt;
&lt;li&gt;Lower latency for small jobs&lt;/li&gt;
&lt;li&gt;Ability to upgrade without disruption&lt;/li&gt;
&lt;li&gt;Scheduling based on actual task resource requirements rather than a count of map and reduce tasks &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Corona的基本组件&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Corona架构&lt;/strong&gt;：
&lt;img alt="Corona" src="http://i.imgur.com/xNagLob.jpg" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Cluster Manager&lt;/strong&gt; 类似于YARN中的Resource Manager，负责资源分配和调度。Cluster Manager掌握着各个节点的资源使用情况，并将资源分配给各个作业（默认调度器为Fair Scheduler）。同YARN中的Resource Manager一样，Resource Manager是一个高度抽象的资源统一分配与调度框架，它不仅可以为MapReduce，也可以为其他计算框架分配资源。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Corona Job Tracker&lt;/strong&gt; 类似于YARN中的Application Master，用于作业的监控和容错，它可以运行在两个模式下：1） 作为JobClient，用于提交作业和方便用户跟踪作业运行状态 2）   作为一个Task运行在某个TaskTracker上。与MRv1中的Job Tracker不同，每个Corona Job Tracker只负责监控一个作业。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Corona Task Tracker&lt;/strong&gt; 类似于YARN中的Node Manager，它的实现重用了MRv1中Task Tracker的很多代码，它通过心跳将节点资源使用情况汇报给Cluster Manager，同时会与Corona Job Tracker通信，以获取新任务和汇报任务运行状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proxy Job Tracker&lt;/strong&gt; 用于离线展示一个作业的历史运行信息，包括Counter、metrics、各个任务运行信息等。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Corona框架的搭建（single node）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona"&gt;https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Building&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start in the hadoop-20 parent path and compile the code with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ant&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Dversion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt; &lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;package&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Starting up the cluster&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, set up the required environment and useful aliases.&lt;/p&gt;
&lt;p&gt;source singleNodeHadoop/singleNodeSwitch.sh corona&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If this is the first time, format the namespace in /tmp with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt; &lt;span class="n"&gt;namenode&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;
&lt;span class="n"&gt;Start&lt;/span&gt; &lt;span class="n"&gt;HDFS&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;
&lt;span class="n"&gt;Start&lt;/span&gt; &lt;span class="n"&gt;Corona&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;corona&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are corresponding commands to stop the clusters
    stop-dfs&lt;/p&gt;
&lt;p&gt;stop-corona&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point you will be able to look at the local Corona Cluster Manager UI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Job&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;export&lt;/span&gt; &lt;span class="n"&gt;HADOOP_CLASSPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;HADOOP_HOME&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;corona&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libthrift&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.7.0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;hadoop&lt;/span&gt; &lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.20&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;sleep&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Dmapred&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fairscheduler&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;group_a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pool_sla&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="Corona"></category><category term="cloud"></category><category term="distribution"></category></entry><entry><title>Network Aware Resource Allocation in Distributed Clouds 笔记</title><link href="http://www.liustrive.com/pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html" rel="alternate"></link><updated>2013-04-06T00:28:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2013-04-06:pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html</id><summary type="html">&lt;p&gt;INFOCOM 2012 &lt;/p&gt;
&lt;p&gt;Mansoor Alicherry, T.V. Lakshman&lt;/p&gt;
&lt;p&gt;Bell Labs. &lt;/p&gt;
&lt;p&gt;这篇文章主要介绍了通讯负载和延迟最小化的高效的resource allocation算法。&lt;/p&gt;
&lt;p&gt;Key contribution：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Develop an efficient 2-approximation algorithm for the optimal selection of data centers in the distributed cloud.&lt;/li&gt;
&lt;li&gt;Use of an optimal algorithm for rack and server selection.&lt;/li&gt;
&lt;li&gt;develop a heuristic for partitioning the requested resources for the task amongst the chosen data centers and racks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;分布式资源调配系统结构：
&lt;img alt="Network Aware Resource Allocation Architecture" src="http://i.imgur.com/FnsGybs.png" /&gt;&lt;/p&gt;
&lt;p&gt;优化的主要目标是减少datacenter之间的traffic，降低communication cost。
Cloud Automation的四个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Datacenter selection&lt;/li&gt;
&lt;li&gt;Request partitioning across datacenters&lt;/li&gt;
&lt;li&gt;Rack, blade and processor selection&lt;/li&gt;
&lt;li&gt;VM placement&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章主要集中于Datacenter selection，即响应用户请求的第一步：找到合适的若干个datacenter来放置合适数量的VM。&lt;/p&gt;
&lt;p&gt;文章提出了近似算法，其主要目标是通过最小路径找到合适的datacenter，并且通过相VM相互间的通讯延迟，避免存在一些性能会严重拉低整体表现的VM。并且算法提供额外的用户约束，如某datacenter中最大、最小VM数量等。&lt;/p&gt;
&lt;p&gt;文章将datacenter选择问题抽象成了一个完全图G = （V,E,w,e）的子图选择问题，即：从完全图中找出一个最长edge最小的子图的问题，称为MINDIAMETER问题。&lt;/p&gt;
&lt;p&gt;需要注意的是这里提出的抽象是针对另一个文章里的MAXCLIQUE 问题改进简化而来，MAXCLIQUE问题是寻找最大clique问题，文中给出的例子如下：
&lt;img alt="Reduction" src="http://i.imgur.com/UU0YP2G.png" /&gt; &lt;/p&gt;
&lt;p&gt;如图中所示，edge上的数字代表link costs，虚线是补全的edge使图成完全图，加粗的线的子图是MINDIAMETER问题的解，这在原图中也满足最大clique。
随后文章提出了近似算法，算法得到最近似的解，该解得到的子图的diameter最多是最优子图的diameter的二倍。&lt;/p&gt;
&lt;p&gt;文章的基础文：&lt;a href="http://dl.acm.org/citation.cfm?id=574848"&gt;Computers and Intractability; A Guide to the Theory of NP-Completeness&lt;/a&gt;，即MAXCLIQUE问题的来源。&lt;/p&gt;
&lt;p&gt;这篇文章提出了考虑网络状况的资源分配算法，其使用的策略跟之前组会提出的相近，划分出一片相互之间延迟最小的子网，把运行任务的VM放在这个子网里。文章提出的近似算法如果能再进一步优化肯定是另一篇A类paper了，涉及的数学和图论功底可能不是我能掌握。但是我觉得如果能想办法结合CPU和内存来放进edge的weight里一起算，应该会得到更好的结果。&lt;/p&gt;</summary><category term="notes"></category><category term="cloud"></category><category term="distribution"></category></entry><entry><title>Yarn初探（一）资源调度</title><link href="http://www.liustrive.com/pages/2013/03/29/yarnchu-tan-zi-yuan-diao-du.html" rel="alternate"></link><updated>2013-03-29T23:45:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2013-03-29:pages/2013/03/29/yarnchu-tan-zi-yuan-diao-du.html</id><summary type="html">&lt;h3&gt;资源调度器&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://i.imgur.com/1pY6g1v.jpg" /&gt;
YARN的资源管理器本质上像一个事件处理器，需要处理6种SchedulerEvent类型的事件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NODE_REMOVED&lt;/strong&gt;：表示集群中被移除一个计算节点，资源调度器收到该事件时需要从可分配资源总量中移除相应的资源量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NODE_ADDED&lt;/strong&gt;：表示集群中增加了一个计算节点，资源调度器收到该事件时需要将新增的资源量添加到可分配资源总量中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;APPLICATION_ADDED&lt;/strong&gt;：表示ResourceManager收到一个新的Application。资源管理器需将该Application添加到相应的数据结构中。通常，资源管理器需要为每个application维护一个独立的数据结构，以便于统一管理和资源分配。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;APPLICATION_REMOVED&lt;/strong&gt;：事件APPLICATION_REMOVED表示一个Application运行结束，资源管理器需将该Application从相应的数据结构中清除。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CONTAINER_EXPIRED&lt;/strong&gt;：当资源调度器将一个container分配给某个ApplicationMaster后，如果该ApplicationMaster在一定时间间隔内没有使用该container，则资源调度器会对该container进行再分配。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NODE_UPDATE&lt;/strong&gt;：NodeManager通过心跳机制向ResourceManager汇报各个container运行情况，会触发一个NODE_UDDATE事件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中，NODE_UPDATE事件发生时表示可能有新的container得到释放，因此该事件会触发资源分配，该事件是6个事件中最重要的事件，它会触发资源调度器最核心的资源分配机制。&lt;/p&gt;
&lt;h3&gt;资源管理与分配&lt;/h3&gt;
&lt;p&gt;相比于MapReduce v1中的资源调度器，YARN采用了事件驱动的模型，因此编写起来更加复杂。同MapReduce v1一样，YARN也自带了三种常用的调度器，分别是FIFO，Capacity Scheduler和Fair Scheduler，其中，第一个是默认的调度器，它属于批处理调度器，而后两个属于多租户调度器，采用树形多队列的形式组织资源。&lt;/p&gt;
&lt;p&gt;YARN的资源表示模型目前只支持CPU和内存的管理与分配，NodeManager启动时，会向ResourceManager注册，而注册信息中会包含该节点可分配的CPU和内存总量，YARN为了更细化的分配CPU资源，将物理CPU划分为多个虚拟CPU，用户提交应用程序时，可以指定每个任务需要的虚拟CPU个数。而对内存资源，YARN采用进程监控的方式控制内存，每个NodeManager会启动一个额外监控线程监控每个container内存资源使用量，一旦发现它超过约定的资源量，则会将其杀死。&lt;/p&gt;
&lt;p&gt;YARN的资源分配模型是以队列的形式组织的，每个用户可属于一个或多个队列，且只能向这些队列中提交application。每个队列被划分了一定比例的资源。而资源分配过程是异步的，资源调度器将资源分配给一个application后，不会立刻push给对应的ApplicaitonMaster，而是暂时放到一个缓冲区中，等待ApplicationMaster通过周期性的RPC函数主动来取，即采用了pull-based模型，而不是push-based模型。&lt;/p&gt;
&lt;h3&gt;有关ApplictionMaster&lt;/h3&gt;
&lt;p&gt;ApplicationManager的三大模块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AMLivelinessMonitor&lt;/strong&gt;：周期性遍历所有ApplicationMaster，如果一个ApplicationMaster在一定时间内未汇报心跳信息，则认为它死掉了，它上面所有正在运行的Container将被置为运行失败，AM本身会被重新分配到另外一个节点上执行。&lt;/p&gt;
&lt;p&gt;对于运行失败的Container, RM不会重新执行，它只会通过心跳机制告诉对应的AM，由AM决定是否重新执行，如果需要，则AM重新向RM申请资源。
- &lt;strong&gt;ApplicationMasterLauncher&lt;/strong&gt;：处理AMLauncherEvent类型的事件，该类型事件有两种，分别是请求启动一个AM的“LAUNCH”和请求清理一个AM的“CLEANUP”。ApplicationMasterLauncher维护了一个线程池，从而能够尽快地处理这两种事件：
- 收到了“LAUNCH”类型的事件，它会与对应的NodeManager通信，要求它启动ApplicationMaster。具体步骤如下：
    1. 创建一个ContainerManager协议的Client；
    2. 向对应的NodeManager发起连接请求；
    3. 启动AM所需的各种信息，并封装成一个StartContainerRequest对象；
    4. 通过RPC函数ContainerManager. startContainer()发送给对应的NM。
- 收到了“CLEANUP”类型的事件，它会与对应的NodeManager通信，要求它杀死ApplicationMaster。整个过程与启动AM的过程类似。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ApplicationMasterService&lt;/strong&gt;：处理来自ApplicationMaster的请求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注册&lt;/li&gt;
&lt;li&gt;心跳&lt;/li&gt;
&lt;li&gt;清理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中，注册是ApplicationMaster启动时发生的行为，请求包中包含AM所在节点，RPC端口号和tracking URL等信息；心跳是周期性行为，包含请求资源的类型描述、待释放的Container列表等，而AMS则为之返回新分配的Container、失败的Container等信息；清理是应用程序运行结束时发生的行为，ApplicationMaster向RM发送清理应用程序的请求，以回收资源和清理各种数据结构。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="Yarn"></category></entry><entry><title>Hello World</title><link href="http://www.liustrive.com/pages/2013/03/14/hello.html" rel="alternate"></link><updated>2013-03-14T19:15:00+08:00</updated><author><name>Leon</name></author><id>tag:www.liustrive.com,2013-03-14:pages/2013/03/14/hello.html</id><summary type="html">&lt;h2&gt;Hello World&lt;/h2&gt;
&lt;p&gt;开个博客，想想还是用 Hello 镇宅&lt;/p&gt;</summary><category term="Hello"></category></entry></feed>