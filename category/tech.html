<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <title>古董Leon</title>
    <meta name="description" content="">
    <meta name="author" content="Leon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="../theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="../theme/bootstrap.min.css" rel="stylesheet">
    <link href="../theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="../theme/local.css" rel="stylesheet">
    <link href="../theme/pygments.css" rel="stylesheet">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="..">古董Leon</a>

        <div class="nav-collapse">
        <ul class="nav">
            
            <li><a href="../pages/about.html">About</a></li>
        </ul>
<form class="navbar-search pull-right" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


    <div class='article'>
        <div class="content-title">
            <a href="../pages/2014/07/26/distributed-representations-of-words-and-phrases-and-their-compositionality-yue-du-bi-ji.html"><h1>Distributed Representations of Words and Phrases and their Compositionality 阅读笔记</h1></a>
2014-07-26

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 
        </div>
        
        <div><h6>via: <a href="www.liustrive.com">liustrive.com</a></h6>
<p>文章提出了一些对Skp-gram模型的扩展，提出一种NCE（Noise Contrastive Estimation ）方法用以代替上文hierarchical softmax方法，从而获得更高的效率和对高频词的更好的词向量结果。
文中提到词向量表示的一大弊端是它们无法表示一些词组成的常用短语，这短语不是字面上把其组成词连接起来的意义，于是提出扩展，将基于词的模型变成基于短语的模型，扩展的原理是先抓出这些短语，然后把这些短语作为整体的“词”加入训练集，然后使用上文实验部分中提到的方法，利用词向量代数运算验证准确性。</p>
<h4><strong>The Skip-gram Model</strong></h4>
<p>上文介绍过skip-gram模型的结构，给定一句由训练词{w1,w2,w3…wT}，其目标是最大化平均对数概率，c为目标词的前后上下文长度
 <div class="math">$$\frac{1}{T}\sum_{t=1}^T \sum_{-c\leq j \leq c, j\neq0} \log p(w_{t+j}|w_t)$$</div>
其中，<span class="math">\(p(w_{t+j} | w_t)\)</span> 用softmax函数计算：
 <div class="math">$$\hat{p}(w_t|w_{t-1},...,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}$$</div>
<br />
即：
 <div class="math">$$p(w_O| w_I) = \frac{exp({v'}_{w_O} {}^{\mathrm{T}} v_{w_I})}{\sum_{w=1}^W exp(v'_w {}^{\mathrm{T}}v_{w_I})}$$</div>
</p>
<h4><strong>Hierarchical Softmax</strong></h4>
<p>Hierarchical Softmax是full softmax的一种计算量优化，其目标是减少计算：<br />
<div class="math">$$p(A|C) = \frac{e^{-E(A,C)}}{\sum_{v=1}^V e^{-E(wv,C)}}$$</div>
时所需的巨大计算量，使其运算量从计算|V|个输出节点变成log|V|
其推导过程如下：<br />
<center><img alt="" src="http://cl.ly/YFsO/cbow-exp_2_1.jpg" /></center></p>
<h4><strong>Negative Sampling</strong></h4>
<p>文章提出一种代替hierarchical softmax的方法（NCE），NCE能够求出softmax的对数概率函数近似最大，由于skip-gram模型只关心词向量的质量，所以在保证词向量准确性的条件下，还可以进一步简化NCE，文章定义了NCG方法：
每个$\log p(W_O | W_I) $的运算被下式替代：<br />
<div class="math">$$\log \sigma ({v'}_{w_O} {}^{\mathrm{T}}v_{w_I}) + \sum_{i=1}^{k} E_{w_i \sim P_n(w)}\log \sigma(-{v'}_{w_i} {}^{\mathrm{T}}v_{w_I})]$$</div>
<br />
其中k是为每个数据样本选用k个负类样本，在小的数据集时可以取5-20，大的数据集甚至取2-5就够用，Negative sampling 和NCE的主要区别是NCE同时需要样本和噪声分布的数值概率 ？  </p>
<h4><strong>Subsampling of Frequent Words</strong></h4>
<p>在大型的语料中，一些词会有非常高的词频，比如the, a , in 这种类型词，但是这种高频词往往比一些低频词提供更少的有效信息，所以为了逆转这种不平衡性，文章提出一种二次抽样方法：<br />
<div class="math">$$P(w_i) = 1-\sqrt{\frac{t}{f(w_i)}}$$</div>
其中<span class="math">\(f(w_i)\)</span>是词<span class="math">\(w_i\)</span>的词频，t是预设的阈值，词频高于t的<span class="math">\(w_i\)</span> 具有<span class="math">\(P(w_i)\)</span>的概率被抛弃。</p>
<h3><strong>实验部分：</strong></h3>
<h4><strong>传统的实验</strong></h4>
<p>这篇文章的实验任务也采用上文的代数运算准确率衡量，实验描述参见上文Task description 
文中实验Skip-gram采用10亿数据量的数据集，抛弃词频少于5次的词，实验结果如图Table1
 <center><img alt="" src="http://cl.ly/YFvs/cbow-exp_2_2.jpg" /></center>  </p>
<p>可以观察到Negative Sampling准确率高于HS算法，比原版的NCE也有少许提升，同时观察到以<span class="math">\(10^{-5}\)</span>为阈值二次采样过滤后，大幅缩短了训练时间，同时少量提升了词向量准确度。</p>
<h4><strong>短语实验</strong></h4>
<p>上文提到过，一些由词组成的固定短语并不是其字面意义，于是作者抓出这些固定短句，并以其为整体加入词库而不是拆分成单个的词，抓出这些词的具体方法是：<br />
<div class="math">$$score(w_i,w_j) = \frac{count(w_i,w_j)-\delta}{count(w_i)\times count(w_j)}$$</div>
基本上就是词以固定形式出现的次数减去一个折扣系数与他们各自出现频率的比值，已决定这个短语的权值，最后权值超过预设的阈值的短语被取出作为固定短语，文中实验采用递降的阈值对训练集跑了2-4次这样的取短语操作，使长固定短语也能被抓出<br />
<center><img alt="" src="http://cl.ly/YGDD/cbow-exp_2_3.jpg" /></center></p>
<p>这里具体怎么个2-4次法，前次的输出作为后次的数据集还是什么，文中没说清。</p>
<p>而后作者设计了针对固定短语的实验，与前文代数运算实验相近，只是测试用例变成：
组成固定短语的词 →固定短语
的关系，采用的样例如Table2：<br />
<center><img alt="" src="http://cl.ly/YF8x/cbow-exp_2_4.jpg" /></center>
根据此类测试用例，实验这对固定短语的词向量代数运算，数据集仍然采用10亿词量大小的Google News，并向词库加入抓取出的固定短语，实验结果发现，HS经过subsampling后具有最好的准确性，这表明subsampling不但能提升训练速度，也能提升一些情况下的准确率。
为了最大化这个固定短语词向量的准确性，另外的实验采用了1000维n=全句的HS算法，并采用330亿词数的数据集，模型准确率达到了72%，而数据集降到6B时，准确率为66%，说明训练数据的规模对词向量的准确率是决定性的。</p>
<h4><strong>Additive Compositionality</strong></h4>
<p>文章最后还讨论了词向量的加法语义合成，如Table4所示：<br />
 <center><img alt="" src="http://cl.ly/YGCO/cbow-exp_2_5.jpg" /></center><br />
词向量加法能得到与相加的两词语义最接近的词或短语，这主要是因为在训练过程中，相加的两词有更大可能与结果短语或词在同一句中，比如“中国”+“首都”= “北京”</p>
<p>文章最后横向与相近工作的其它模型做了比较，比较训练时间以及词向量的准确率，结果表明Skip-gram phrase model 具有最快的训练时间（随着数据集规模的增长只有少量时间增加）和很好的准确性。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
        <hr />
    </div>
		

 
        

 

    <div class='article'>
        <a href="../pages/2014/07/25/efficient-estimation-of-word-representations-in-vector-space-yue-du-bi-ji.html"><h2>Efﬁcient Estimation of Word Representations in Vector Space 阅读笔记</h2></a>
        <div class= "well small"> 2014-07-25

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h6>via: <a href="www.liustrive.com">liustrive.com</a></h6>
<h4><strong>背景</strong></h4>
<p>现有模型，比如前文Bengio的采用非线性激活函数tanh的神经网络模型，虽然准确度上有优势，但是算法的复杂度较高，无法训练更大的数据集，比如特征向量维度为50-100情况下训练超过几百兆个的语料。</p>
<h4><strong>目标</strong></h4>
<p>提出一种模型使其能够训练百万级词表下数十亿词量的训练集，并产出优质词特征向量。</p>
<h4><strong>Model Architectures </strong></h4>
<h5><strong>Feedforward Neural Net Language Model (NNLM)</strong></h5>
<p>此即前文所述前向神经网络语言模型，采用四层神经网络（原文讲实际是三层神经网络）
输入层、投影层、隐藏层、输出层
每个训练样本的计算复杂度为：
<div class="math">$$Q = N × D + N × D × H + H × V$$</div>
其中，N为n-gram模型中n值，D为词向量维度，H是隐藏层size（通常500-1000），V是词表大小，同时也是输出层的节点数量，N<em>D为输入层到投影层权重个数，H</em>V表示隐藏层到输出层的权重个数，N<em>D ...</em></p> <a class="btn btn-info xsmall" href="../pages/2014/07/25/efficient-estimation-of-word-representations-in-vector-space-yue-du-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html"><h2>A Neural Probabilistic Language Model 入门笔记</h2></a>
        <div class= "well small"> 2014-07-12

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h5>Via:  <a href="www.liustrive.com">liusrive.com</a></h5>
<hr />
<h2><strong>目标：</strong></h2>
<p><strong>learn the joint probability function of sequences of words in a language</strong></p>
<p>学习某种语言中单词序列的联合概率函数
即根据训练集按照一定的算法建立一种数学模型，使之能够计算出某个句子在该模型下出现的概率，以及根据前N-1个词决定第N个词出现的概率。</p>
<p><em>注：见背景知识：语言模型及n-gram 模型</em></p>
<p><strong>该目标面临的主要困难：</strong><br />
Curse of dimensionality 维数灾难<br />
具体表现为：<br />
<em>a word sequence on which the model will be tested is likely to be different from all the word sequences ...</em></p> <a class="btn btn-info xsmall" href="../pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2013/05/12/corona-bi-ji.html"><h2>Corona 笔记</h2></a>
        <div class= "well small"> 2013-05-12

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h3>Corona 初探</h3>
<p>本周关注了一下另一种号称下一代MapReduce的框架：<strong>Corona</strong>。它是facebook的开源框架，代码在：</p>
<p><a href="https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona">https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona</a> </p>
<p>Cornna的设计需求与Apache的YARN框架十分相似，它的发布文档：</p>
<p>“<a href="http://www.jorditorres.org/wp-content/uploads/2012/11/CC-MEI-Corona-AntonioRodriguez.pdf">Under the Hood: Scheduling MapReduce jobs more efficiently with Corona</a>”</p>
<p>其中提到，当前Map-Reduce所使用的single Job Tracker在Facebook的运行环境下已经遇到瓶颈，在Hadoop Corona中，集群的资源是由central Cluster Manager统一分配调度，每个job拥有各自单独的Corona Job Tracker，这些与YARN框架的设计思路非常相近，解决了Map-Reduce中存在的扩展性、单点故障等不足。</p>
<p>相比于YARN，Corona似乎更贴近Map-Reduce的原版，其代码在原版Map-Reduce基础上修改而来，似乎更贴近原有Map-Reduce的用户；另一大优势在于Corona所采用的Push-based通信机制，在原版Map-Reduce和YARN中，采用轮询方式使client获取信息 ...</p> <a class="btn btn-info xsmall" href="../pages/2013/05/12/corona-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html"><h2>Network Aware Resource Allocation in Distributed Clouds 笔记</h2></a>
        <div class= "well small"> 2013-04-06

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><p>INFOCOM 2012 </p>
<p>Mansoor Alicherry, T.V. Lakshman</p>
<p>Bell Labs. </p>
<p>这篇文章主要介绍了通讯负载和延迟最小化的高效的resource allocation算法。</p>
<p>Key contribution：</p>
<ol>
<li>Develop an efficient 2-approximation algorithm for the optimal selection of data centers in the distributed cloud.</li>
<li>Use of an optimal algorithm for rack and server selection.</li>
<li>develop a heuristic for partitioning the requested resources for the task ...</li></ol> <a class="btn btn-info xsmall" href="../pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="../category/tech.html">1</a></li>
    <li class=""><a href="../category/tech2.html">2</a></li>

    <li class="next"><a href="../category/tech2.html">Next &rarr;</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="../archives.html">Archives</a>
                <li><a href="../tags.html">Tags</a>
                <li><a href="http://www.liustrive.com/feeds/all.rss.xml" rel="alternate">RSS feed</a></li>
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="../category/life.html">life</a></li>
                <li><a href="../category/tech.html">Tech</a></li>
                   
            </ul>
            </div>




            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="http://weibo.com/LiuLianZJU2SJTU">微博</a></li>
                <li><a href="https://www.facebook.com/liustrive">Facebook</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="..">古董Leon</a> &copy; Leon 2014</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="http://twitter.github.com/bootstrap/assets/js/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-50000386-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>