<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <title>古董Leon</title>
    <meta name="description" content="">
    <meta name="author" content="Leon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="../theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="../theme/bootstrap.min.css" rel="stylesheet">
    <link href="../theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="../theme/local.css" rel="stylesheet">
    <link href="../theme/pygments.css" rel="stylesheet">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="..">古董Leon</a>

        <div class="nav-collapse">
        <ul class="nav">
            
            <li><a href="../pages/about.html">About</a></li>
        </ul>
<form class="navbar-search pull-right" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


    <div class='article'>
        <div class="content-title">
            <a href="../pages/2014/07/29/word2vec-code-yue-du-bi-ji.html"><h1>Word2vec Code 阅读笔记</h1></a>
2014-07-29

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 
        </div>
        
        <div><h6>via: <a href="http://www.liustrive.com">liustrive.com</a></h6>
<h3><strong>参数：</strong></h3>
<div class="highlight"><pre><span class="n">hs</span>
<span class="n">negative</span>
<span class="kt">int</span> <span class="n">binary</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cbow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">window</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num_threads</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">min_reduce</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="c1">// window是训练时前后文长度随机数的边界，其它参数看名字</span>
<span class="n">layer1_size</span>  <span class="c1">// 向量维度</span>
</pre></div>


<h3><strong>结构：</strong></h3>
<p>基本词结构体</p>
<div class="highlight"><pre><span class="k">struct</span> <span class="n">vocab_word</span> <span class="p">{</span>
  <span class="kt">long</span> <span class="kt">long</span> <span class="n">cn</span><span class="p">;</span> <span class="c1">// 词频数</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">point</span><span class="p">;</span>   <span class="c1">// root 到该节点的路径，存的是路径节点索引</span>
  <span class="kt">char</span> <span class="o">*</span><span class="n">word</span><span class="p">,</span> <span class="c1">// 词串</span>
<span class="o">*</span><span class="n">code</span><span class="p">,</span> <span class="c1">// Huffman编码</span>
<span class="n">codelen</span><span class="p">;</span> <span class="c1">// Huffman码长度</span>
<span class="p">};</span>
</pre></div>


<h3><strong>全局变量：</strong></h3>
<p><strong>struct vocab_word *vocab; // 词表</strong>   </p>
<p>值得注意的是词表的内存是1000为单位递增扩大的，相关代码：  </p>
<div class="highlight"><pre><span class="k">if</span> <span class="p">(</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="n">vocab_max_size</span><span class="p">)</span> <span class="p">{</span>
<span class="n">vocab_max_size</span> <span class="o">+=</span> <span class="mi">1000</span><span class="p">;</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">vocab_word</span> <span class="o">*</span><span class="p">)</span><span class="n">realloc</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">vocab_max_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="k">struct</span> <span class="n">vocab_word</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>


<p><strong>int *vocab_hash; // 词hash码对应词下标的数组</strong><br />
 vocab_hash[hashcode] –&gt; index<br />
需要注意的是word2vec采用的是标准hash table存放方式，hash码重复后挨着放
取的时候根据拿出index找到词表里真正单词，跟比一下  </p>
<p><strong>syn0</strong> <br />
就是词向量的大矩阵，第i行表示vocab中下标为i的词  </p>
<p><strong>syn1</strong> <br />
用hs算法时用到的辅助矩阵，即文章中的Wx </p>
<p><strong>syn1neg </strong><br />
negative sampling算法时用到的辅助矩阵</p>
<p><strong>Next_random</strong><br />
作者自己生成的随机数，线程里面初始化就是：</p>
<div class="highlight"><pre><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="n">next_random</span> <span class="o">=</span> <span class="p">(</span><span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="n">id</span><span class="p">;</span>
</pre></div>


<p>更新用的：</p>
<div class="highlight"><pre><span class="n">next_random</span> <span class="o">=</span> <span class="n">next_random</span> <span class="o">*</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="mi">25214903917</span> <span class="o">+</span> <span class="mi">11</span><span class="p">;</span>
</pre></div>


<p>伪随机伪到家了=_=  </p>
<p><strong>expTable</strong><br />
直接用一个数组记录指数值，要用的时候直接查近似值，不用算，其初始化代码如下：</p>
<div class="highlight"><pre>  <span class="n">expTable</span> <span class="o">=</span> <span class="p">(</span><span class="n">real</span> <span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">((</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">real</span><span class="p">));</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">EXP_TABLE_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp</span><span class="p">((</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">real</span><span class="p">)</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">MAX_EXP</span><span class="p">);</span> 
    <span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>         <span class="c1">// Precompute f(x) = x / (x + 1)</span>
  <span class="p">}</span>
</pre></div>


<p><strong>vocab_size</strong><br />
 词表大小</p>
<h3><strong>函数: </strong></h3>
<p><strong>void InitNet()</strong><br />
主要承担初始化、开内存、建Huffman树的任务，为syn0,syn1（采用HS时）,syn1neg（采用negative时）开内存，其中syn0用-0.005~0.005之间的数随机初始化</p>
<p><strong>void CreateBinaryTree()</strong><br />
创建Huffman树，以词频为权重，于是词频越高的词Huffman code越短
基本流程与Huffman建树算法一致，但是代码很优雅</p>
<p><strong>void <em>TrainModelThread(void </em>id)</strong><br />
SubSample 相关代码<br />
<center><img alt="" src="http://cl.ly/YHt5/codeblock_1.jpg" /></center><br />
一定概率扔掉高频词，词频越高discard概率越大。</p>
<h3><strong>代码</strong></h3>
<p>neu1 隐层矩阵 neu1[i]为隐层向量，</p>
<p><strong>CBOW</strong><br />
b = next_random % window; <br />
<center><img alt="" src="http://cl.ly/YHYn/codeblock_2.jpg" /></center>
CBOW模型采取的是将一个随机大小的窗口中文本向量相加获得隐层向量</p>
<p><strong>HS</strong><br />
<center><img alt="" src="http://cl.ly/YHhB/codeblock_3.jpg" /></center><br />
其中，f是 <span class="math">\(σ(neu1^T  ×syn1)\)</span> ，l2是父节点索引<br />
<span class="math">\(f \leftarrow p(x)\)</span><br />
交叉熵L为： <br />
<span class="math">\(xlogp(x)+(1-x)*log(1-p(x))\)</span><br />
其中<br />
<span class="math">\(p(x) =\frac{exp(neu1[c] * syn1[c + l2])}{(1+exp(neu1[c] * syn1[c + l2]))}\)</span><br />
于是有：<br />
<div class="math">$$log(L) = code[d]* neu1[c] * syn1[c + l2] –(1-code[d])*log(1 + exp(neu1[c] * syn1[c + l2]))$$</div>
<br />
求对syn1的偏导：<br />
<div class="math">$$\frac{(∂log(L))}{∂syn1}= (1 -code - p(x))*neu1$$</div>
<br />
对neu1求偏导：<br />
<div class="math">$$neu1e=  \frac{(∂log(L))}{∂neu1}= (1 -code - p(x))*syn1$$</div>
</p>
<p>所以g是梯度乘以学习速率中的公因子</p>
<p><strong>Negative Sampling 代码</strong>
 <center><img alt="" src="http://cl.ly/YIE4/codeblock_4.jpg" /></center>
采取negative次生成负样本，当negative为0时选word自己作为正样本，当抽到word自己时跳过<br />
抽取的负样本label为0，可以看做HS中的一层
于是有<br />
<div class="math">$$交叉熵 = -label \times \log f - (1-label)\times (1-f)$$</div>
</p>
<p><strong>隐藏层到输入层：</strong><br />
<center><img alt="" src="http://cl.ly/YHcD/codeblock_5.jpg" /></center> <br />
前面用的输入层向量相加得到隐层向量，于是对词向量的更新可以直接用梯度neu1e[c]</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
        <hr />
    </div>
		

 
        

 

    <div class='article'>
        <a href="../pages/2014/07/27/linguistic-regularities-in-continuous-space-word-representations-yue-du-bi-ji.html"><h2>Linguistic Regularities in Continuous Space Word Representations 阅读笔记</h2></a>
        <div class= "well small"> 2014-07-27

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h6>via: <a href="http://www.liustrive.com">liustrive.com</a></h6>
<p>这篇文章主要就是介绍词向量支持基本代数运算的特性，并利用这一特性将之应用到SemEval 2012 Task 来衡量。<br />
<strong>背景：</strong><br />
<a href="http://www.cs.york.ac.uk/semeval-2012/index.php?id=tasks">SemEval 2012 task</a></p>
<h3><strong>Recurrent Neural Network Model</strong></h3>
<p>RNNLM模型由输入层、带有循环连接以及对应权重矩阵的隐藏层、输出层构成，如下图：<br />
<center><img alt="" src="http://cl.ly/YFxq/RNN.jpg" /></center>
输入是词表V大小|V|维向量，W(t)表示时间t输入的用1-of-N编码表示的词，输出层y(t)产生词的概率分布，维度也是|V|，隐藏层s(t)维护一个句子出现历史的描述，隐层到输出层采用如下计算：<br />
<center><img alt="" src="http://cl.ly/YIRB/cbow-exp_2_6.jpg" /></center><br />
训练产出中词向量保存在参数矩阵U中，模型训练采用后向传播的最大化对数似然函数方法。</p>
<h3><strong>实验部分</strong></h3>
<h4><strong>测试集</strong></h4>
<p><strong>Syntactic Test Set</strong><br />
词向量的语法规则测试集如下图Table1<br />
<center><img alt="" src="http://cl.ly/YIAi/cbow-exp_2_7.jpg" /></center></p>
<p>作者采用Penn Treebank POS tags对报纸新闻文本做标记，选取词频最高的前100组（JJ/JJR ...</p> <a class="btn btn-info xsmall" href="../pages/2014/07/27/linguistic-regularities-in-continuous-space-word-representations-yue-du-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2014/07/26/distributed-representations-of-words-and-phrases-and-their-compositionality-yue-du-bi-ji.html"><h2>Distributed Representations of Words and Phrases and their Compositionality 阅读笔记</h2></a>
        <div class= "well small"> 2014-07-26

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h6>via: <a href="http://www.liustrive.com">liustrive.com</a></h6>
<p>文章提出了一些对Skp-gram模型的扩展，提出一种NCE（Noise Contrastive Estimation ）方法用以代替上文hierarchical softmax方法，从而获得更高的效率和对高频词的更好的词向量结果。
文中提到词向量表示的一大弊端是它们无法表示一些词组成的常用短语，这短语不是字面上把其组成词连接起来的意义，于是提出扩展，将基于词的模型变成基于短语的模型，扩展的原理是先抓出这些短语，然后把这些短语作为整体的“词”加入训练集，然后使用上文实验部分中提到的方法，利用词向量代数运算验证准确性。</p>
<h4><strong>The Skip-gram Model</strong></h4>
<p>上文介绍过skip-gram模型的结构，给定一句由训练词{w1,w2,w3…wT}，其目标是最大化平均对数概率，c为目标词的前后上下文长度
 <div class="math">$$\frac{1}{T}\sum_{t=1}^T \sum_{-c\leq j \leq c, j\neq0} \log p(w_ ...</div></p> <a class="btn btn-info xsmall" href="../pages/2014/07/26/distributed-representations-of-words-and-phrases-and-their-compositionality-yue-du-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2014/07/25/efficient-estimation-of-word-representations-in-vector-space-yue-du-bi-ji.html"><h2>Efﬁcient Estimation of Word Representations in Vector Space 阅读笔记</h2></a>
        <div class= "well small"> 2014-07-25

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h6>via: <a href="http://www.liustrive.com">liustrive.com</a></h6>
<h4><strong>背景</strong></h4>
<p>现有模型，比如前文Bengio的采用非线性激活函数tanh的神经网络模型，虽然准确度上有优势，但是算法的复杂度较高，无法训练更大的数据集，比如特征向量维度为50-100情况下训练超过几百兆个的语料。</p>
<h4><strong>目标</strong></h4>
<p>提出一种模型使其能够训练百万级词表下数十亿词量的训练集，并产出优质词特征向量。</p>
<h4><strong>Model Architectures </strong></h4>
<h5><strong>Feedforward Neural Net Language Model (NNLM)</strong></h5>
<p>此即前文所述前向神经网络语言模型，采用四层神经网络（原文讲实际是三层神经网络）
输入层、投影层、隐藏层、输出层
每个训练样本的计算复杂度为：
<div class="math">$$Q = N × D + N × D × H + H × V$$</div>
其中，N为n-gram模型中n值，D为词向量维度，H是隐藏层size（通常500-1000），V是词表大小，同时也是输出层的节点数量，N<em>D为输入层到投影层权重个数，H</em>V表示隐藏层到输出层的权重个数，N<em>D ...</em></p> <a class="btn btn-info xsmall" href="../pages/2014/07/25/efficient-estimation-of-word-representations-in-vector-space-yue-du-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html"><h2>A Neural Probabilistic Language Model 入门笔记</h2></a>
        <div class= "well small"> 2014-07-12

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h5>Via:  <a href="http://www.liustrive.com">liusrive.com</a></h5>
<hr />
<h2><strong>目标：</strong></h2>
<p><strong>learn the joint probability function of sequences of words in a language</strong></p>
<p>学习某种语言中单词序列的联合概率函数
即根据训练集按照一定的算法建立一种数学模型，使之能够计算出某个句子在该模型下出现的概率，以及根据前N-1个词决定第N个词出现的概率。</p>
<p><em>注：见背景知识：语言模型及n-gram 模型</em></p>
<p><strong>该目标面临的主要困难：</strong><br />
Curse of dimensionality 维数灾难<br />
具体表现为：<br />
<em>a word sequence on which the model will be tested is likely to be different from all the word sequences ...</em></p> <a class="btn btn-info xsmall" href="../pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="../author/leon.html">1</a></li>
    <li class=""><a href="../author/leon2.html">2</a></li>

    <li class="next"><a href="../author/leon2.html">Next &rarr;</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="../archives.html">Archives</a>
                <li><a href="../tags.html">Tags</a>
                <li><a href="http://www.liustrive.com/feeds/all.rss.xml" rel="alternate">RSS feed</a></li>
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="../category/life.html">life</a></li>
                <li><a href="../category/tech.html">Tech</a></li>
                   
            </ul>
            </div>




            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="http://weibo.com/LiuLianZJU2SJTU">微博</a></li>
                <li><a href="https://www.facebook.com/liustrive">Facebook</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="..">古董Leon</a> &copy; Leon 2014</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="http://twitter.github.com/bootstrap/assets/js/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-50000386-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>