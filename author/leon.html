<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <title>古董Leon</title>
    <meta name="description" content="">
    <meta name="author" content="Leon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="../theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="../theme/bootstrap.min.css" rel="stylesheet">
    <link href="../theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="../theme/local.css" rel="stylesheet">
    <link href="../theme/pygments.css" rel="stylesheet">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="..">古董Leon</a>

        <div class="nav-collapse">
        <ul class="nav">
            
            <li><a href="../pages/about.html">About</a></li>
        </ul>
<form class="navbar-search pull-right" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


    <div class='article'>
        <div class="content-title">
            <a href="../pages/2014/07/12/a-neural-probabilistic-language-model-ru-men-bi-ji.html"><h1>A Neural Probabilistic Language Model 入门笔记</h1></a>
2014-07-12

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 
        </div>
        
        <div><h5>Via:  <a href="www.liustrive.com">liusrive.com</a></h5>
<hr />
<h4><strong>目标：</strong></h4>
<p><strong>learn the joint probability function of sequences of words in a language</strong></p>
<p>学习某种语言中单词序列的联合概率函数
即根据训练集按照一定的算法建立一种数学模型，使之能够计算出某个句子在该模型下出现的概率，以及根据前N-1个词决定第N个词出现的概率。</p>
<p><em>注：见背景知识：语言模型及n-gram 模型</em></p>
<p><strong>该目标面临的主要困难：</strong><br />
Curse of dimensionality 维数灾难<br />
具体表现为：<br />
<em>a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training</em><br />
被测试的词经常并未出现在训练集中，对应概率是0，模型无法准确处理，即模型的泛化能力低</p>
<p><strong>传统的解决办法：</strong><br />
<em>concatenating very short overlapping sequences seen in the training set</em><br />
采用n-gram模型，文中提到了两种常用方法：  </p>
<ul>
<li>回退法（back-off trigram models）　　
    n-gram 值为0，则采用n-1 gram并乘以一个权重模拟  </li>
<li>平滑法（smoothed trigram models）<br />
有多种平滑方法，比较基础的是为未出现的词或n元组赋一个非零值，并为出现过的词或n元组递增同样的值，比如为unseen的n-gram赋1，并将所有seen n-gram加1参见<a href="http://en.wikipedia.org/wiki/N-gram#Smoothing_techniques">wiki</a></li>
</ul>
<p><em>注：参见背景知识：基本平滑算法</em>  </p>
<p><strong>以上方法存在一些问题：</strong></p>
<ul>
<li><em>it is not taking into account contexts farther than 1 or 2 words (截止文章发表时)</em>  </li>
<li><em>it is not taking into account the “similarity” between words</em></li>
</ul>
<p>传统的方法不能建立更长的关系，无法训练出高阶语言模型；
同时，并未考虑词的相似性，相似的词往往可以出现在相同的句式，比如：</p>
<blockquote>
<p>The cat is walking in the bedroom
A dog was running in a room</p>
</blockquote>
<p><strong>本文提出的方法：</strong>
 <img alt="文中方法" src="https://dl-web.dropbox.com/get/public_Markdown/NN_paper_pic1.jpg?_subject_uid=106627057&amp;w=AADUureOCFAfmR4n-C4WR7Gmz4YLB20mnToz2QtbKdMXww" />
将词典中的词用词特征向量表示，通过这些特征向量来表示次序列的联合概率模型，并在模型训练阶段通过最大化训练数据的对数似然函数同时训练好特征向量和概率模型的参数。</p>
<h4><strong>A Neural Model</strong></h4>
<p>训练集:
sequence w1…wT ，其中的词wt 属于于很大的有限集词表V
目标模型： 
$$f(w_t,...,w_{t=n+1}) = \hat{P}(w_t | w_1^{t-1})$$
将目标分解为两部分：
<center> <img alt="" src="https://dl-web.dropbox.com/get/public_Markdown/NNLM.jpg?_subject_uid=106627057&amp;w=AADmsFt-4kWJLEQyYOu-vF32eM64ckysBHZ3ekLtO4cqDg" />
图1 三层神经网络</center></p>
<ol>
<li>将词表中的元素i映射为distributed feature vectors，即特征向量C(i)，C是一个|V|*m的矩阵，m为特征向量维数，C(i)为C中一行;</li>
<li>根据上下文生成的概率分布函数g，其输入为前n词的 (C(wt−n+1),••• ,C(wt−1))，输出为一个向量，其第i的元素表示 $\hat{P}(w_t = i|w_1^{t-1})$
$$f(i,w_{t-1},...,w_{t-1-n+1} = g(i,C(w_{t-1},...,C(w_{t-n+1})))$$</li>
</ol>
<p>函数f由两个映射关系g和C构成，C的参数就是|V|<em>m矩阵表示的词特征向量，w到C(w)的映射表示从矩阵C取出一行，C和g的参数 $θ= (C,ω)$ 构成模型需要学习的参数集，训练的过程是寻找能够使训练语料的penalized log-likelihood最大的$θ$，学习过程中参数数量和词表大小、上下文数量n成线性关系。
 $$L=\frac{1}{T}\sum_t log f(w_t,w_{t-1},...,w_{t-n+1};\theta)+ R(\theta)$$
文中使用的实际包含一个隐藏层的神经网络进行训练，输出层共|V|个节点, 采用softmax激活函数将y归一化成概率公式：
 $$\hat{P}(w_t|w_{t-1},...,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}$$
</em>注：此处公式参见背景知识：神经网络*</p>
<p>其中，$y_i$ 为下一个词为i的未归一化log概率，隐藏层使用d+Hx得到d为偏置项，具有参数b,W,U,d,H： 
 $$y=b+Wx+U tanh(d+Hx)$$
输入层x为将C(wt−1),C(wt−2),••• ,C(wt−n+1)这些向量首尾相连形成的（n-1）m维向量：
 $$x=(C(w_{t-1}),C(w_{t-2}),...,C(w_{t-n+1}))$$
于是需要训练的参数集合为：
 $$\theta = (b,d,W,U,H,C)$$
其中，U为一个|V|<em>h矩阵，表示隐藏层到输出层的参数，矩阵W为一个|V|</em>（n-1）m的矩阵，包含了输入层到输出层的直连边，（参见图1中虚线以及正文末算法）。</p>
<p>最后通过随机梯度下降法(SGD)优化模型，最终获得词向量和语言模型。
 $$\theta \leftarrow \theta +  \varepsilon\frac{\partial log\hat{P}(w_t|w_{t-1},...,w_{t-n+1})}{\partial \theta}$$</p>
<h4><strong>Parallel Implementation</strong></h4>
<p>虽然前文提到本文模型参数增长为线性增长，但由于采用n-gram模型时，计算P(wt|wt−1,...,wt−n+1) 时并不需要计算词表中每一个词的概率，神经网络模型所需的计算量远大于n-gram模型。
有关并行处理，在共享内存多处理机上文中算法实现首先采用每个处理器处理一部分子数据集，update模型的参数时需要等待资源锁，结果导致大部分CPU周期用来等待其它处理器释放参数的写锁；文章随后采用了异步实现，任何处理器任何时间都可以写共享内存，这样的实现会导致一些处理器update的参数被其它处理器写线程覆盖，但是实验表明这种noise不会降低训练速度。
文章最终使用fast network cluster作为环境，采用了parallelize across the parameters 的方式进行计算，每个CPU负责计算输出的部分子集的unnormalized probability，并且更新涉及到的输出单元参数。</p>
<p>这种策略下，CPU间的通讯只需要包括:  </p>
<ul>
<li>output softmax的归一化因子</li>
<li>隐藏层和词特征层的梯度</li>
</ul>
<p>对于单一训练样本，假设有词表数|V|，隐藏单元h，词序n，词特征项m，则计算复杂度为： |V|(1+nm+h)+h(1+nm)+nm
单一训练样本复杂度除以计算weighted sums of output units复杂度得到：
 $$\frac{|V|(1+(n-1)m+h)}{|V|(1+(n-1)m+h)+h(1+(n-1)m)+(n-1)m}$$
可知总体上将输出单元计算并行化是有优势的。</p>
<h4><strong>处理器i，数据点t 的计算(BP)</strong></h4>
<p><strong>Forward phase，前向计算：</strong>
1. 为词特征层前向计算
    &gt;   $x(k) \leftarrow C(w_{t-k})$
    &gt;   $x=(x(1),x(2),...,x(n-1))$  </p>
<p>2. 为隐藏层前向计算，d是偏置项，激活函数tanh
    &gt; $o \leftarrow d + Hx$
    &gt;    $a \leftarrow tanh(o)$</p>
<p>3. 为第i个块内输出做前向计算</p>
<blockquote>
<p>$s_i \leftarrow 0$
   for each j in i-th block
    &gt;   　$y_j \leftarrow b_j + a.U_j$
    &gt;   　if( direct connections) 
    &gt;   　　$y_j \leftarrow y_j + x.W_j$<br />
    &gt;   　$p_j \leftarrow e^{y_j}$
    &gt;   　$S_i \leftarrow S_i + p_j$ </p>
</blockquote>
<p>$S=∑_i s_i$，此计算需在所有处理器间共享$S_i$</p>
<p>4. 正规化概率</p>
<blockquote>
<p>对第i块内输出以j为下标循环
$p_j \leftarrow p_j/S$</p>
</blockquote>
<p>5. 更新对数似然函数</p>
<p>$$$$<strong>Backward/update phase 后向计算/更新阶段</strong>
1. 清空梯度向量∂L/∂a，∂L/∂x</p>
<blockquote>
<p>对第i个块内以j为下标循环
  　$\frac{\partial L}{\partial y_j} \leftarrow 1_{j==w_t - p_j}$
 　$b_j \leftarrow b_j + \varepsilon \frac{\partial L}{\partial y_j}$
 　If(direct connections)
 　　$ \frac{\partial L}{\partial x} \leftarrow \frac{\partial L}{\partial x} + \frac{\partial L}{\partial y_j}W_j$
　$ \frac{\partial L}{\partial a} \leftarrow \frac{\partial L}{\partial a} + \frac{\partial L}{\partial y_j}W_j$
　If(direct connection)
　　$W_j \leftarrow W_j + \varepsilon\frac{\partial L}{\partial y_j}x$
　$U_j \leftarrow U_j + \varepsilon\frac{\partial L}{\partial y_j}x$</p>
</blockquote>
<p>2. 对$\frac{\partial L}{\partial a}, \frac{\partial L}{\partial x}$求和并在所有处理器上共享
3. 后向传播并更新隐藏层权重</p>
<blockquote>
<p>k由1到h
　$\frac{\partial L}{\partial o_k} \leftarrow (1 -a_k^2)\frac{\partial L}{\partial a_k}$
　$\frac{\partial L}{\partial x} \leftarrow \frac{\partial L}{\partial x} + H'\frac{\partial L}{\partial o}$
　$d \leftarrow d + \varepsilon \frac{\partial L }{\partial o}$
　$H \leftarrow H + \varepsilon \frac{\partial L }{\partial o}x'$</p>
</blockquote>
<p>4. 为输入更新此特征向量</p>
<blockquote>
<p>$K ∈ (1,n-1)$
$C(w_{t-k} \leftarrow C(w_{t-k}) + \varepsilon \frac{\partial L}{\partial x(k)})$
 其中$∂L/(∂x(k))$是∂$L/∂x$的第k分量</p>
</blockquote>
<h4><strong>背景知识</strong></h4>
<h5><strong>语言模型及n-gram 模型</strong></h5>
<p><strong>语言模型：</strong>
统计式的语言模型是借由一个机率分布，而指派机率给字词所组成的字串：
 $$P(w_1,...,w_m)$$
由此得到推论：
$$P(w_1,w_2,…,w_t)=P(w_1)×P(w_2|w_1)×P(w_3|w_1,w_2)×…×P(w_t|w_1,w_2,…,w_{t−1})$$
简单的讲就是看一串字词的组合有多大可能出现，换句话讲就是有没有可能是正常人类语言。</p>
<p><strong>N-gram模型：</strong>
语言的一种统计学模型为，根据已出现的词，得出下一个词出现的概率，即：
 $$\hat{p}(w_1^T)\prod_{t=1}^T \hat{p}(w_t|w_1^{t-1})$$
在建立自然语言的统计学模型时，为简化问题，假设第t个词只与其前n-1个词有关，于是有：
 $$\hat{p}(w_t|w_1^{t-1})\approx \hat{p}(w_t|w_{t-n+1}^{t-1})$$
根据这个模型，通过对语料建立一个多项分布模型，采用最大似然估计，可以获得任意句子在该模型下出现的概率。
 $$p(w_i|w_1w_2...w_{i-1}) = \frac{C(w_1w_2...w_{i-1}w_i)}{C(w_1w_2...w_{i-1})}$$</p>
<h5><strong>基本平滑算法</strong></h5>
<ul>
<li>回退Back-off
    如果n-gram的值为零，则用n-1 gram来计算  </li>
<li>平滑Smoothing
    将MLE方法与其它方向相混合，保证没有0概率的值</li>
<li>加1平滑
    T:训练数据,V:词表,w: 词
          预测 p’(w|h)=(c(h,w)+1)/(c(h)+|V|)
          特别:非条件分布时p’(w)=(c(w)+1)/(|T|+|V|)
    问题：经常会|V|&gt;c(h)，甚至|V|&gt;&gt;c(h) </li>
<li>小于1平滑：
    加入λ系数 
    -T:训练数据,V:词表,w: 词
       预测 p’(w|h)=(c(h,w)+λ)/(c(h)+ λ |V|), λ&lt;1
        特别:非条件分布时p’(w)=(c(w)+ λ)/(|T|+ λ |V|)</li>
<li>
<p>Good-Turing:
    1、适用于评估大规模的数据
    2、相似点：
    $pr(w)=(c(w)+1)<em>N(c(w)+1)/(|T|</em>N(c(w)))$,
    其中：N(c)是数目为c的词的数量
    特别：对于训练集中没有出现的词，$c(w)=0 pr(w)=N(1)/(|T|*N(0))$
    有利于数量少的词语（&lt;5-10, 但N(c)较大）
    “调富济贫”
    归一化（可以得到$\lambda_i &gt; 0, \sum_{i=0...n} \lambda_i = 1$）</p>
</li>
<li>
<p>典型n-gram语言模型的平滑:<br> <br />
 1、采用$λ=(λ0,λ1,λ2,λ3)$ <br> 
 2、归一化：
       $λi &gt; 0, ∑_{(i=0…n)}λ_i=1 $<br>
    3、极大似然估计
    : 固定p3,p2,p1和|V|，根据训练数据确定参数
    : 再寻找一组{λi}使得交叉熵达到最小
    （使数据的概率达到最大）：</p>
</li>
<li>
<p>EM平滑算法:
1、从某些λ开始，如λj&gt;0,所有j∈0..3
2、计算每个λj的期望数值 
3、采用“Next λ”公式计算的λj新集合 
4、返回2，除非遇到终止条件 
    终止条件为： λ收敛 
      简单设定一个ε，当step3中对每个j都有 
      $|λj-λj ,next|&lt; ε$时终止 
 </p>
</li>
</ul>
<h5><strong>神经网络</strong></h5>
<p>神经网络的本质是两个阶段非线性统计模型：
<center> <img alt="" src="https://dl-web.dropbox.com/get/public_Markdown/NN_note_basic.jpg?_subject_uid=106627057&amp;w=AABR3UpMBmqifCYORPcB75lSGTH3RZPa4k5715iTB8GuFA" />
图2神经网络模型</center>
如图2所示，中间红色Z为导出特征，也称隐藏层，有输入的线性组合创建Z，再用Z的线性组合建立以Y为目标的模型。
 $$Z_m = \sigma(\alpha_{0m} + \alpha_m^TX),m=1,...,M$$
 $$T_k = \beta_{0k} + \beta_k^TZ, k=1,...,K$$
 $$f_k(X) = g_k(T), k=1,...,K$$
激活函数$σ（ע）=exp(y)/(1+exp(y))=1/(1+exp(-y))$ 
输出函数g_k (T)是对向量T的最终变换，早期K分类采用恒等函数，后来被softmax函数取代，因其可以产生和为1的正估计：
 $$g_k(T) = \frac{e^{T_k}}{\sum_{\epsilon=1}^{K}e^{T_\epsilon}}$$</p>
<h5><strong>Training of Neural Networks</strong></h5>
<p>未知参数称为权，用$\theta$表示权的全集 
对于回归和分类问题，我们分别使用误差的平方和，平方误差或互熵（离散）作为拟合的度量 
 $${\alpha_{0m},\alpha_m;m=1,2,...,M} M(p+1) weights$$
 $${\beta_{0k},\beta_k;k=1,2,...,K} K(M+1) weights$$
 $$R(\theta) = \sum_{k=1}^K \sum_{i=1}^{N}(y_{ik}-f_k(x_i))^2$$
  $$R(\theta) = - \sum_{i=1}^N \sum_{k=1}^{K}(y_{ik}-\log f_k(x_i))$$</p><script type= "text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
</div>
        <hr />
    </div>
		

 
        

 

    <div class='article'>
        <a href="../pages/2013/05/12/corona-bi-ji.html"><h2>Corona 笔记</h2></a>
        <div class= "well small"> 2013-05-12

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h3>Corona 初探</h3>
<p>本周关注了一下另一种号称下一代MapReduce的框架：<strong>Corona</strong>。它是facebook的开源框架，代码在：</p>
<p><a href="https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona">https://github.com/facebook/hadoop-20/tree/master/src/contrib/corona</a> </p>
<p>Cornna的设计需求与Apache的YARN框架十分相似，它的发布文档：</p>
<p>“<a href="http://www.jorditorres.org/wp-content/uploads/2012/11/CC-MEI-Corona-AntonioRodriguez.pdf">Under the Hood: Scheduling MapReduce jobs more efficiently with Corona</a>”</p>
<p>其中提到，当前Map-Reduce所使用的single Job Tracker在Facebook的运行环境下已经遇到瓶颈，在Hadoop Corona中，集群的资源是由central Cluster Manager统一分配调度，每个job拥有各自单独的Corona Job Tracker，这些与YARN框架的设计思路非常相近，解决了Map-Reduce中存在的扩展性、单点故障等不足。</p>
<p>相比于YARN，Corona似乎更贴近Map-Reduce的原版，其代码在原版Map-Reduce基础上修改而来，似乎更贴近原有Map-Reduce的用户；另一大优势在于Corona所采用的Push-based通信机制，在原版Map-Reduce和YARN中，采用轮询方式使client获取信息 ...</p> <a class="btn btn-info xsmall" href="../pages/2013/05/12/corona-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html"><h2>Network Aware Resource Allocation in Distributed Clouds 笔记</h2></a>
        <div class= "well small"> 2013-04-06

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><p>INFOCOM 2012 </p>
<p>Mansoor Alicherry, T.V. Lakshman</p>
<p>Bell Labs. </p>
<p>这篇文章主要介绍了通讯负载和延迟最小化的高效的resource allocation算法。</p>
<p>Key contribution：</p>
<ol>
<li>Develop an efficient 2-approximation algorithm for the optimal selection of data centers in the distributed cloud.</li>
<li>Use of an optimal algorithm for rack and server selection.</li>
<li>develop a heuristic for partitioning the requested resources for the task ...</li></ol> <a class="btn btn-info xsmall" href="../pages/2013/04/06/network-aware-resource-allocation-in-distributed-clouds-bi-ji.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2013/03/29/yarnchu-tan-zi-yuan-diao-du.html"><h2>Yarn初探（一）资源调度</h2></a>
        <div class= "well small"> 2013-03-29

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h3>资源调度器</h3>
<p><img alt="" src="http://i.imgur.com/1pY6g1v.jpg" />
YARN的资源管理器本质上像一个事件处理器，需要处理6种SchedulerEvent类型的事件：</p>
<ul>
<li><strong>NODE_REMOVED</strong>：表示集群中被移除一个计算节点，资源调度器收到该事件时需要从可分配资源总量中移除相应的资源量。</li>
<li><strong>NODE_ADDED</strong>：表示集群中增加了一个计算节点，资源调度器收到该事件时需要将新增的资源量添加到可分配资源总量中。</li>
<li><strong>APPLICATION_ADDED</strong>：表示ResourceManager收到一个新的Application。资源管理器需将该Application添加到相应的数据结构中。通常，资源管理器需要为每个application维护一个独立的数据结构，以便于统一管理和资源分配。</li>
<li><strong>APPLICATION_REMOVED</strong>：事件APPLICATION_REMOVED表示一个Application运行结束，资源管理器需将该Application从相应的数据结构中清除。</li>
<li><strong>CONTAINER_EXPIRED</strong>：当资源调度器将一个container分配给某个ApplicationMaster后，如果该ApplicationMaster在一定时间间隔内没有使用该container，则资源调度器会对该container进行再分配。</li>
<li><strong>NODE_UPDATE</strong>：NodeManager通过心跳机制向ResourceManager汇报各个container运行情况，会触发一个NODE_UDDATE事件。</li>
</ul>
<p>其中，NODE_UPDATE事件发生时表示可能有新的container得到释放，因此该事件会触发资源分配，该事件是6个事件中最重要的事件，它会触发资源调度器最核心的资源分配机制。</p>
<h3>资源管理与分配</h3>
<p>相比于MapReduce v1中的资源调度器，YARN采用了事件驱动的模型，因此编写起来更加复杂。同MapReduce v1一样，YARN也自带了三种常用的调度器，分别是FIFO，Capacity Scheduler和Fair Scheduler，其中，第一个是默认的调度器，它属于批处理调度器，而后两个属于多租户调度器，采用树形多队列的形式组织资源。</p>
<p>YARN的资源表示模型目前只支持CPU和内存的管理与分配，NodeManager启动时，会向ResourceManager注册 ...</p> <a class="btn btn-info xsmall" href="../pages/2013/03/29/yarnchu-tan-zi-yuan-diao-du.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="../pages/2013/03/14/hello.html"><h2>Hello World</h2></a>
        <div class= "well small"> 2013-03-14

by <a class="url fn" href="../author/leon.html">Leon</a>
 


 </div>
        <div class="summary"><h2>Hello World</h2>
<p>开个博客，想想还是用 Hello 镇宅</p> <a class="btn btn-info xsmall" href="../pages/2013/03/14/hello.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="../author/leon.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="../archives.html">Archives</a>
                <li><a href="../tags.html">Tags</a>
                <li><a href="http://www.liustrive.com/feeds/all.rss.xml" rel="alternate">RSS feed</a></li>
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="../category/life.html">life</a></li>
                <li><a href="../category/tech.html">Tech</a></li>
                   
            </ul>
            </div>




            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="http://weibo.com/LiuLianZJU2SJTU">微博</a></li>
                <li><a href="https://www.facebook.com/liustrive">Facebook</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="..">古董Leon</a> &copy; Leon 2014</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="http://twitter.github.com/bootstrap/assets/js/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-50000386-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>